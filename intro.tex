\documentclass[runningheads]{llncs}
\usepackage{amsfonts}

\newtheorem{mytheorem}{Theorem}
\newtheorem{mylemma}[mytheorem]{Lemma}
\newtheorem{myquestion}[mytheorem]{Question}
\newtheorem{myexample}[mytheorem]{Example}
\newtheorem{myproposition}[mytheorem]{Proposition}
\newtheorem{mycorollary}[mytheorem]{Corollary}
\newtheorem{mydefinition}[mytheorem]{Definition}
\newtheorem{myprinciple}[mytheorem]{Principle}

\begin{document}

\title{Extending environments to incentivize self-reflection in reinforcement learning}
\titlerunning{Extending environments to incentivize self-reflection}

\author{Samuel Allen Alexander\inst{1}\orcidID{0000-0002-7930-110X}}
\authorrunning{S.\ A.\ Alexander}
\institute{The U.S.\ Securities and Exchange Commission
\email{samuelallenalexander@gmail.com}
\url{https://philpeople.org/profiles/samuel-alexander/publications}}

\maketitle

\begin{abstract}
    Fill this in.
\end{abstract}

\section{Introduction}

In building the road to AGI, we must appeal AGI's unique properties.
Indeed, if we built a road using only more general properties, which hold of other things
as well as of AGIs, then how could it be a road to AGI? It would be a road to a
more general class of things, not to AGI. Take reinforcement learning (RL) for example, in
which an agent interacts with a certain type of reward-giving environment (hereafter called
a \emph{traditional RL environment}).
An AGI could interact with such
a traditional RL environment, but so could various other creatures: humans, animals,
human societies. If there is nothing unique about how an AGI interacts with
traditional RL environments,
then how could traditional RL environments possibly lead to AGI?
Thinking along these lines motivated us to consider
special extended environments in which AGIs can participate, but in which humans or animals
can not realistically participate.

AGIs (as we envision them here) have a key property which uniquely allows them to
participate in extended environments in which other actors could not practically
participate. Namely, we envision an AGIs as acting deterministically according to
a well-defined computer program. This is crucial because, unlike a human or animal
actor, such a computer program can be perfectly simulated and forked. Whereas a
traditional RL environment chooses rewards and updates states based solely on the
actions which an agent takes, our extended environments will have access to the AGI's
source-code, allowing the environment to choose rewards and states based on, for example,
what the agent \emph{would have done} in various situations. For example, a chess-playing
environment, when deciding whether to make a move which would potentially expose it to
checkmate, can, as part of its deliberations, simulate the AGI opponent in order to
determine whether or not the AGI opponent would take advantage of said opportunity.

We will give examples of extended environments designed to incentivize RL agents to
recursively engage in self-reflection, in the sense that any agent who does not so
engage could not possibly perform well in said environments.
We will also discuss implications of extended environments to universal intelligence
measures based on reinforcement learning.

\section{Examples of Self-reflection-incentivizing Environments}

\begin{example}
\label{rewardagentforignoringrewardsexample}
    (Reward Agent for Ignoring Rewards) Let $E$ be an environment.
    We define an extended environment $E'$ as follows.
    \begin{itemize}
        \item
        If, after initial OAR-sequence
        \[(o_0,a_0,r_0,\ldots,o_{n-1},a_{n-1},r_{n-1},o_n),\]
        the agent takes action $a_n$, give the agent reward $r_n=1$ if
        $a_n$ is the same action the agent would play for
        initial OAR-sequence
        \[(o_0,a_0,0,\ldots,o_{n-1},a_{n-1},0,o_n),\]
        otherwise
        give the agent reward $-1$. Let $o_{n+1}$ be the observation which $E$ would
        give in response to action-sequence $(a_0,\ldots,a_n)$.
    \end{itemize}
\end{example}

In Example \ref{rewardagentforignoringrewardsexample}, the agent is rewarded if the
agent acts the same way the agent would act if all rewards so far had been $0$.
Otherwise, the agent is punished. Thus, paradoxically, the agent is rewarded for
ignoring rewards. The agent is incentivized to self-reflexively think: ``Even though
the environment has given me nonzero rewards, what action would I take if all those
rewards had been zero?''

\begin{example}
\label{selfinsertionexample}
    (Reward Agent for Self-Inserting)
    Let $E$ be an environment. We define an extended environment $E'$
    as follows.
    \begin{itemize}
        \item
        Suppose, after initial OAR-sequence
        \[(o_0,a_0,r_0,\ldots,o_{n-1},a_{n-1},r_{n-1},o_n),\]
        %\[(a_0,r_0,o_0,\ldots,a_{n-1},r_{n-1},o_{n-1}),\]
        the agent takes action $a_n$. For $0<i\leq n$, interpret $o_i$ as
        encoding a pair $o_i=\langle r'_{i-1},o'_i\rangle$.
        Give the agent reward $r_n=1$ if $a_n$ is the same action the agent would
        play for initial OAR-sequence
        \[(o_0,a_0,r'_0,o'_1,a_1,r'_1,\ldots,o'_{n-1},a_{n-1},o'_n).\]
        %\[(a_0,r'_0,o'_0,\ldots,a_{n-1},r'_{n-1},o'_{n-1}).\]
        Otherwise give the agent reward $r_n=-1$.
        Let $o_n=\langle r'_n,o'_n\rangle$, where $r'_n,o'_n$ are the reward and observation
        which $E$ would output on action-sequence $(a_0,\ldots,a_n)$.
    \end{itemize} 
\end{example}

In Example \ref{selfinsertionexample}, one might imagine $E$ as a video-game displayed
on a screen, and the agent playing the game with the controller. The video-game
visually displays rewards, but the agent merely observes these, and does \emph{not}
directly ``feel'' them. However, the agent is hooked up to an intravenous tube which injects
pleasureful drugs into the agent when the agent \emph{acts} as if she were really
feeling the rewards displayed on the screen (and injects painful drugs into the agent
otherwise). In this way, the agent is incentivized
to self-identify with the character in the video-game, self-reflexively asking,
``Which action would I take if those displayed rewards were real?''

\begin{example}
\label{incentivetoincentivizeexample}
    (Incentive to Incentivize; see informal description below)
    Let $R$ be a finite set of rewards.
    Suppose, after initial OAR-sequence
    \[(o_0,a_0,r_0,\ldots,o_{n-1},a_{n-1},r_{n-1},o_n)\]
    %$(a_0,r_0,o_0,\ldots,a_{n-1},r_{n-1},o_{n-1})$
    (where each $a_i\in R$ is a reward considered as an action),
    the agent takes action $a_n\in R$ (a reward considered as an action).
    Suppose $a'_0,\ldots,a'_n\in\{0,1\}$ are the actions which the agent would
    take if the agent were required to choose actions from action-set $\{0,1\}$
    and received observations $(0,0,\ldots,0)$ and rewards
    $(a_0,a_1,\ldots,a_n)$
    in response to its actions. Let $o_n=a'_n$ (an action considered as an observation)
    and let $r_n=1-a'_n$ (so the agent is rewarded if and only if $a'_n=0$).
\end{example}

In Example \ref{incentivetoincentivizeexample}, the agent observes
actions (from action-set $\{0,1\}$), and the agent must act by
choosing rewards for those observed actions.
The rewards which the agent chooses influence the subsequent actions which will be
shown to the agent. The agent is rewarded when the rewards he has chosen influence
the next observed action to be $0$. Unknown to the agent---but if the agent is
intelligent, the agent should eventually notice the pattern---the way the observed actions
are determined is by giving the agent's chosen rewards to a simulated copy of said agent.
Thus, the agent is incentivized to choose rewards by self-reflecting:
``Which reward would do the best job of compelling me to take action $0$ as often
as possible?'' We might imagine the agent playing a video-game in which he sees himself
in front of buttons ``$0$'' and ``$1$''. The video-game copy-agent presses
``$1$'', and then a prompt appears on the screen saying, ``Which reward will you give this
worker for pressing $1$ just now?'' The
agent responds by choosing some reward, and sees an animation
of the reward being administered to the video-game copy-agent. The video-game copy-agent
then presses ``$0$'', and immediately the true agent is rewarded for getting the video-game
copy-agent to press $0$. Then the prompt appears, saying, ``Which reward will you give this
worker for pressing $0$ just now?'' And so on forever.

Example \ref{incentivetoincentivizeexample} would be more natural if the action-set were
not restricted to a \emph{finite} set of rewards.
That restriction was only made to bring the example in conformance with the usual
finite-action convention.
The example could be modified by allowing
the agent to notate rewards by typing on a keyboard (one action per key-press) or something
similar\footnote{As Wang and Hammer say: ``Decision makings often do not happen
at the level of basic operations, but at the level of composed actions, where
there are usually infinite possibilities'' \cite{wang2015assumptions}.}.
The example is also interesting in that the agent, desiring the clone to take action $0$
as often as possible, is incentivized to choose harsh punishments when the clone takes
action $1$. If punishments are limited to real numbers, then the agent might face a dilemma
similar to one that arises when traditional RL is considered for cancer treatment applications.
An RL surgeon should be punished with an infinitely large negative reward for killing
the patient, but this is impossible if rewards are restricted to real numbers
\cite{wirth2017survey} \cite{zhao2009reinforcement}. Similarly, the agent in Example
\ref{incentivetoincentivizeexample} would probably like to choose infinitely large
punishments, if possible, when its clone from takes action $1$. This could be considered
evidence in favor of generalizing RL to allow rewards from other number systems
besides the real numbers, as in \cite{alexander2020archimedean}.

\begin{example}
\label{selfrecognitionexample}
    (Incentivizing Self-recognition)
    Let $s_0,s_1,\ldots$ be an enumeration of all OAR-sequences with award-set
    $\{0,1\}$. We define an environment $E$ as follows.
    Suppose, after initial OAR-sequence
    \[(o_0,a_0,r_0,\ldots,o_{n-1},a_{n-1},r_{n-1},o_n),\]
    in which each $o_i=s_i$,
    the agent takes action $a_n\in\{0,1\}$. If $o_n$ is consistent with the agent
    (meaning that the agent would take the actions in $s_n$ in response to the observations
    and rewards in $s_n$), then give the agent reward $r_n=a_n$.
    Otherwise, give the agent reward $1-a_n$. Let $o_n=s_n$.
\end{example}

In Example \ref{selfrecognitionexample}, the agent is shown various OAR-sequences, and for
each sequence, the agent either pushes button ``Looks like me'' ($1$)
or ``Doesn't look like me'' ($0$). For each OAR-sequence, the agent is rewarded if and
only if the agent correctly guessed whether or not the OAR-sequence is an OAR-sequence
which that agent would produce, given the observations and rewards in the sequence.
Thus, the agent is incentivized to self-reflect in order to ask, ``If I experienced the
observations and rewards in that OAR-sequence, would I act that way?''

\begin{example}
\label{otheraspectsexample}
    (Recognizing other aspects of oneself)
    All the below environments are similar to Example \ref{selfrecognitionexample},
    and we describe them informally to avoid technical details.
    \begin{itemize}
        \item
        (Supervised learning)
        Assume there is a canonical, computable function $f$ which transforms
        each RL agent $A$ into a supervised learning agent $f(A)$. By a \emph{supervised
        learning trial} we mean quadruple $\langle L,T,I,p\rangle$ where $L$ is a finite set
        of labels, $T$ is a sequence of images with labels from $L$ (a \emph{training set}),
        $I$ is an unlabeled image, and $p:L\to \mathbb Q\cap [0,1]$ is a function
        assigning to each label $\ell\in L$ a probability that $\ell$ is the correct label
        for $I$. We define an extended environment as follows.
        The agent $A$ is sytematically shown all supervised learning trials and must
        take action $1$ (``Looks like me'') or $0$ (``Doesn't look like me''), and is
        rewarded or punished accordingly depending whether or not $f(A)$ would
        output $p$ in response to $I$ after being trained with $T$.
        \item
        (Unsupervised learning)
        Assume there is a canonical, computable function $g$ which transforms each RL
        agent $A$ into an unsupervised learning agent $g(A)$.
        By an \emph{unsupervised learning trial} we mean a triple
        $\langle n,D,C\rangle$ where $n$ is a positive integer, $D\subseteq \mathbb Q^n$
        is a finite set of $n$-dimensional points with rational coordinates, and $C$
        is a clustering of $D$.
        We define an extended environment as follows. The agent $A$ is systematically
        shown all unsupervised learning trials and must take action $1$ (``Looks like me'')
        or $0$ (``Doesn't look like me''), and is rewarded or punished accordingly depending
        whether or not $g(A)$ would cluster $D$ into clustering $C$.
        \item
        (The Turing Test)
        Assume there is a canonical, computable function $h$ which transforms each RL
        agent $A$ into an English-speaking chatbot $h(A)$.
        By a \emph{chatbot trial} we mean a sequence of strings of English characters.
        We define an extended environment as follows. The agent $A$ is systematically
        shown all chatbot trials and must take action $1$ (``Looks like me'') or $0$
        (``Doesn't look like me''), and is rewarded or punished accordingly depending
        whether or not even-numbered strings in the chatbot trial are what $h(A)$ would
        say in response to the user saying the odd-numbered strings.
        \item
        (Adversarial sequence prediction) Similar to the above environments, assuming
        a canonical computable function which transforms each RL agent into a predictor
        in the game of adversarial sequence prediction \cite{hibbard2008adversarial}
        \cite{hibbard}.
        \item
        (Mechanical knowing agent) Assume there is a canonical, computable function
        $i$ which transforms each RL agent $A$ into a code $i(A)$ of a computably
        enumerable set of sentences in the language of Epistemic Arithmetic
        \cite{shapiro}; $i(A)$ is thought of as a mechanical knowing agent
        \cite{carlson}. We define an extended environment as follows. The agent $A$
        is systematically shown all sentences in the language of Epistemic Arithmetic,
        with repetition, in such a way that each sentence is shown infinitely often.
        Upon being shown sentence $\phi$ for the $n$th time, the agent must take action
        $1$ (``I know $\phi$ is true'')
        or $0$ (``I'm not sure if $\phi$ is true'')
        and is rewarded if and only if $\phi$ is
        enumerated by $i(A)$ in $\leq n$ steps of computation.
    \end{itemize}
\end{example}

The extended environments in Example \ref{otheraspectsexample} incentivize the agent
to self-reflect, asking itself questions like ``Does that look like how I would
classify that image, given that training set?'' or ``Does that look like how I would
cluster that set of points?'' or ``Does that conversation participant say the same things
I would say?'' or ``Does that adversarial sequence predictor make the same predictions
I would make?'' or ``Does that mechanical agent know the same things I know?''

\bibliographystyle{splncs04}
\bibliography{intro}

\end{document}