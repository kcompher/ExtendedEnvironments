\documentclass[runningheads]{llncs}

\newtheorem{mytheorem}{Theorem}
\newtheorem{mylemma}[mytheorem]{Lemma}
\newtheorem{myquestion}[mytheorem]{Question}
\newtheorem{myexample}[mytheorem]{Example}
\newtheorem{myproposition}[mytheorem]{Proposition}
\newtheorem{mycorollary}[mytheorem]{Corollary}
\newtheorem{mydefinition}[mytheorem]{Definition}
\newtheorem{myprinciple}[mytheorem]{Principle}

\begin{document}

\title{Extending environments to incentivize self-reflection in reinforcement learning}
\titlerunning{Extending environments to incentivize self-reflection}

\author{Samuel Allen Alexander\inst{1}\orcidID{0000-0002-7930-110X}}
\authorrunning{S.\ A.\ Alexander}
\institute{The U.S.\ Securities and Exchange Commission
\email{samuelallenalexander@gmail.com}
\url{https://philpeople.org/profiles/samuel-alexander/publications}}

\maketitle

\begin{abstract}
    Fill this in.
\end{abstract}

\section{Introduction}

In building the road to AGI, we must appeal AGI's unique properties.
Indeed, if we built a road using only more general properties, which hold of other things
as well as of AGIs, then how could it be a road to AGI? It would be a road to a
more general class of things, not to AGI. Take reinforcement learning (RL) for example, in
which an agent interacts with a certain type of reward-giving environment (hereafter called
a \emph{traditional RL environment}).
An AGI could interact with such
a traditional RL environment, but so could various other creatures: humans, animals,
human societies. If there is nothing unique about how an AGI interacts with
traditional RL environments,
then how could traditional RL environments possibly lead to AGI?
Thinking along these lines motivated us to consider
special extended environments in which AGIs can participate, but in which humans or animals
can not realistically participate.

AGIs (as we envision them here) have a key property which uniquely allows them to
participate in extended environments in which other actors could not practically
participate. Namely, we envision an AGIs as acting deterministically according to
a well-defined computer program. This is crucial because, unlike a human or animal
actor, such a computer program can be perfectly simulated and forked. Whereas a
traditional RL environment chooses rewards and updates states based solely on the
actions which an agent takes, our extended environments will have access to the AGI's
source-code, allowing the environment to choose rewards and states based on, for example,
what the agent \emph{would have done} in various situations. For example, a chess-playing
environment, when deciding whether to make a move which would potentially expose it to
checkmate, can, as part of its deliberations, simulate the AGI opponent in order to
determine whether or not the AGI opponent would take advantage of said opportunity.

We will give examples of extended environments designed to incentivize RL agents to
recursively engage in self-reflection, in the sense that any agent who does not so
engage could not possibly perform well in said environments.
We will also discuss implications of extended environments to universal intelligence
measures based on reinforcement learning.

\section{Examples of Self-reflection-incentivizing Environments}

\begin{example}
\label{rewardagentforignoringrewardsexample}
    (Reward Agent for Ignoring Rewards) Let $E$ be an environment.
    We define an extended environment $E'$ as follows.
    \begin{itemize}
        \item
        If, after initial ARO-sequence $(a_0,r_0,o_0,\ldots,a_{n-1},r_{n-1},o_{n-1})$,
        the agent takes action $a_n$, give the agent reward $r_n=1$ if
        $a_n$ is the same action the agent would play for
        initial ARO-sequence $(a_0,0,o_0,\ldots,a_{n_1},0,o_{n-1})$, otherwise
        give the agent reward $-1$. Let $o_n$ be the observation which $E$ would
        give in response to action-sequence $(a_0,\ldots,a_n)$.
    \end{itemize}
\end{example}

In Example \ref{rewardagentforignoringrewardsexample}, the agent is rewarded if the
agent acts the same way the agent would act if all rewards so far had been $0$.
Otherwise, the agent is punished. Thus, paradoxically, the agent is rewarded for
ignoring rewards. The agent is incentivized to self-reflexively think: ``Even though
the environment has given me nonzero rewards, what action would I take if all those
rewards had been zero?''

\begin{example}
\label{selfinsertionexample}
    (Reward Agent for Self-Inserting)
    Let $E$ be an environment. We define an extended environment $E'$
    as follows.
    \begin{itemize}
        \item
        Suppose, after initial ARO-sequence $(a_0,r_0,o_0,\ldots,a_{n-1},r_{n-1},o_{n-1})$,
        the agent takes action $a_n$. For $i=0,\ldots,n-1$, interpret $o_i$ as
        encoding a pair $o_i=\langle r'_i,o'_i\rangle$.
        Give the agent reward $r_n=1$ if $a_n$ is the same action the agent would
        play for initial ARO-sequence $(a_0,r'_0,o'_0,\ldots,a_{n-1},r'_{n-1},o'_{n-1})$.
        Otherwise give the agent reward $r_n=-1$.
        Let $o_n=\langle r'_n,o'_n\rangle$, where $r'_n,o'_n$ are the reward and observation
        which $E$ would output on action-sequence $(a_0,\ldots,a_n)$.
    \end{itemize} 
\end{example}

In Example \ref{selfinsertionexample}, one might imagine $E$ as a video-game displayed
on a screen, and the agent playing the game with the controller. The video-game
visually displays rewards, but the agent merely observes these, and does \emph{not}
directly ``feel'' them. However, the agent is hooked up to an IV drip which injects
pleasureful drugs into the agent when the agent \emph{acts} as if she were really
feeling the rewards displayed on the screen. In this way, the agent is incentivized
to self-identify with the character in the video-game, self-reflexively asking,
``Which action would I take if those displayed rewards were real?''

\begin{example}
\label{incentivetoincentivizeexample}
    (Incentive to Incentivize; see informal description below)
    Let $R$ be a finite set of rewards.
    Suppose, after initial ARO-sequence
    $(a_0,r_0,o_0,\ldots,a_{n-1},r_{n-1},o_{n-1})$
    (where each $a_0\in R$ is a reward considered as an action),
    the agent takes action $a_n\in R$ (a reward considered as an action).
    Suppose $a'_0,\ldots,a'_n\in\{0,1\}$ are the actions which the agent would
    take if the agent were required to choose actions from action-set $\{0,1\}$
    and received reward-observation pairs $(a_0,0,a_1,0,\ldots,a_n,0)$
    in response to its actions. Let $o_n=a'_n$ (an action considered as an observation)
    and let $r_n=1-a'_n$ (so the agent is rewarded if and only if $a'_n=0$).
\end{example}

In Example \ref{incentivetoincentivizeexample}, the agent observes
actions (from action-set $\{0,1\}$), and the agent must act by
choosing rewards for those observed actions.
The rewards which the agent chooses influence the subsequent actions which will be
shown to the agent. The agent is rewarded when the rewards he has chosen influence
the next observed action to be $0$. Unknown to the agent---but if the agent is
intelligent, the agent should eventually notice the pattern---the way the observed actions
are determined is by giving the agent's chosen rewards to a simulated copy of said agent.
Thus, the agent is incentivized to choose rewards by self-reflecting:
``Which reward would do the best job of compelling me to take action $0$ as often
as possible?'' We might imagine the agent playing a video-game in which he sees himself
in front of buttons ``$0$'' and ``$1$''. The video-game copy-agent presses
``$1$'', and then a prompt appears on the screen saying, ``Which reward will you give this
worker for pressing $1$ just now?'' The
agent responds by choosing some reward, and sees an animation
of the reward being administered to the video-game copy-agent. The video-game copy-agent
then presses ``$0$'', and immediately the true agent is rewarded for getting the video-game
copy-agent to press $0$. Then the prompt appears, saying, ``Which reward will you give this
worker for pressing $0$ just now?'' And so on forever.

Example \ref{incentivetoincentivizeexample} would be more natural if the action-set were
not restricted to a \emph{finite} set of rewards.
That restriction was only made to bring the example in conformance with the usual
finite-action convention.
The example could be modified by allowing
the agent to notate rewards by typing on a keyboard (one action per key-press) or something
similar\footnote{As Wang and Hammer say: ``Decision makings often do not happen
at the level of basic operations, but at the level of composed actions, where
there are usually infinite possibilities'' \cite{wang2015assumptions}.}.
The example is also interesting in that the agent, desiring the clone to take action $0$
as often as possible, is incentivized to choose harsh punishments when the clone takes
action $1$. If punishments are limited to real numbers, then the agent might face a dilemma
similar to one that arises when traditional RL is considered for cancer treatment applications.
An RL surgeon should be punished with an infinitely large negative reward for killing
the patient, but this is impossible if rewards are restricted to real numbers
\cite{wirth2017survey} \cite{zhao2009reinforcement}. Similarly, the agent in Example
\ref{incentivetoincentivizeexample} would probably like to choose infinitely large
punishments, if possible, when its clone from takes action $1$. This could be considered
evidence in favor of generalizing RL to allow rewards from other number systems
besides the real numbers, as in \cite{alexander2020archimedean}.

\bibliographystyle{splncs04}
\bibliography{intro}

\end{document}