\documentclass[runningheads]{llncs}

\newtheorem{mytheorem}{Theorem}
\newtheorem{mylemma}[mytheorem]{Lemma}
\newtheorem{myquestion}[mytheorem]{Question}
\newtheorem{myexample}[mytheorem]{Example}
\newtheorem{myproposition}[mytheorem]{Proposition}
\newtheorem{mycorollary}[mytheorem]{Corollary}
\newtheorem{mydefinition}[mytheorem]{Definition}
\newtheorem{myprinciple}[mytheorem]{Principle}

\begin{document}

\title{Extending environments to incentivize self-reflection in reinforcement learning}
\titlerunning{Extending environments to incentivize self-reflection}

\author{Samuel Allen Alexander\inst{1}\orcidID{0000-0002-7930-110X}}
\authorrunning{S.\ A.\ Alexander}
\institute{The U.S.\ Securities and Exchange Commission
\email{samuelallenalexander@gmail.com}
\url{https://philpeople.org/profiles/samuel-alexander/publications}}

\maketitle

\begin{abstract}
    Fill this in.
\end{abstract}

\section{Introduction}

In building the road to AGI, we must appeal AGI's unique properties.
Indeed, if we built a road using only more general properties, which hold of other things
as well as of AGIs, then how could it be a road to AGI? It would be a road to a
more general class of things, not to AGI. Take reinforcement learning (RL) for example, in
which an agent interacts with a certain type of reward-giving environment (hereafter called
a \emph{traditional RL environment}).
An AGI could interact with such
a traditional RL environment, but so could various other creatures: humans, animals,
human societies. If there is nothing unique about how an AGI interacts with
traditional RL environments,
then how could traditional RL environments possibly lead to AGI?
Thinking along these lines motivated us to consider
special extended environments in which AGIs can participate, but in which humans or animals
can not realistically participate.

AGIs (as we envision them here) have a key property which uniquely allows them to
participate in extended environments in which other actors could not practically
participate. Namely, we envision an AGIs as acting deterministically according to
a well-defined computer program. This is crucial because, unlike a human or animal
actor, such a computer program can be perfectly simulated and forked. Whereas a
traditional RL environment chooses rewards and updates states based solely on the
actions which an agent takes, our extended environments will have access to the AGI's
source-code, allowing the environment to choose rewards and states based on, for example,
what the agent \emph{would have done} in various situations. For example, a chess-playing
environment, when deciding whether to make a move which would potentially expose it to
checkmate, can, as part of its deliberations, simulate the AGI opponent in order to
determine whether or not the AGI opponent would take advantage of said opportunity.

We will give examples of extended environments designed to incentivize RL agents to
recursively engage in self-reflection, in the sense that any agent who does not so
engage could not possibly perform well in said environments.
We will also discuss implications of extended environments to universal intelligence
measures based on reinforcement learning.

\section{Examples of Self-reflection-incentivizing Environments}

\begin{example}
\label{rewardagentforignoringrewardsexample}
    (Reward Agent for Ignoring Rewards) Let $E$ be a traditional RL environment.
    We define an extended environment $E'$ as follows.
    \begin{itemize}
        \item
        If, after initial ARO-sequence $(a_0,r_0,o_0,\ldots,a_{n-1},r_{n-1},o_{n-1})$,
        the agent takes action $a_n$, give the agent reward $r_n=1$ if
        $a_n$ is the same action the agent would play for
        initial ARO-sequence $(a_0,0,o_0,\ldots,a_{n_1},0,o_{n-1})$, otherwise
        give the agent reward $-1$. Let $o_n$ be the observation which $E$ would
        give in response to action-sequence $(a_0,\ldots,a_n)$.
    \end{itemize}
\end{example}

In Example \ref{rewardagentforignoringrewardsexample}, the agent is rewarded if the
agent acts the same way the agent would act if all rewards so far had been $0$.
Otherwise, the agent is punished. Thus, paradoxically, the agent is rewarded for
ignoring rewards. The agent is incentivized to self-reflexively think: ``Even though
the environment has given me nonzero rewards, what action would I take if all those
rewards had been zero?''

\begin{example}
\label{selfinsertionexample}
    (Reward Agent for Self-Inserting)
    Let $E$ be a traditional RL environment. We define an extended environment $E'$
    as follows.
    \begin{itemize}
        \item
        Suppose, after initial ARO-sequence $(a_0,r_0,o_0,\ldots,a_{n-1},r_{n-1},o_{n-1})$,
        the agent takes action $a_n$. For $i=0,\ldots,n-1$, interpret $o_i$ as
        encoding a pair $o_i=\langle r'_i,o'_i\rangle$.
        Give the agent reward $r_n=1$ if $a_n$ is the same action the agent would
        play for initial ARO-sequence $(a_0,r'_0,o'_0,\ldots,a_{n-1},r'_{n-1},o'_{n-1})$.
        Otherwise give the agent reward $r_n=-1$.
        Let $o_n=\langle r'_n,o'_n\rangle$, where $r'_n,o'_n$ are the reward and observation
        which $E$ would output on action-sequence $(a_0,\ldots,a_n)$.
    \end{itemize} 
\end{example}

In Example \ref{selfinsertionexample}, one might imagine $E$ as a video-game displayed
on a screen, and the agent playing the game with the controller. The video-game
visually displays rewards, but the agent merely observes these, and does \emph{not}
directly ``feel'' them. However, the agent is hooked up to an IV drip which injects
pleasureful drugs into the agent when the agent \emph{acts} as if she were really
feeling the rewards displayed on the screen. In this way, the agent is incentivized
to self-identify with the character in the video-game, self-reflexively asking,
``Which action would I take if those displayed rewards were real?''



\bibliographystyle{splncs04}
\bibliography{intro}

\end{document}