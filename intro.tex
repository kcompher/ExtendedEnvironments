\documentclass[runningheads]{llncs}
\usepackage{amsfonts}
\usepackage{amsmath}

\newtheorem{mytheorem}{Theorem}
\newtheorem{mylemma}[mytheorem]{Lemma}
\newtheorem{myquestion}[mytheorem]{Question}
\newtheorem{myexample}[mytheorem]{Example}
\newtheorem{myproposition}[mytheorem]{Proposition}
\newtheorem{mycorollary}[mytheorem]{Corollary}
\newtheorem{mydefinition}[mytheorem]{Definition}
\newtheorem{myprinciple}[mytheorem]{Principle}

\begin{document}

\title{Extending environments to incentivize self-reflection
in deterministic reinforcement learning}
\titlerunning{Extending environments to incentivize self-reflection}

\author{Samuel Allen Alexander\inst{1}\orcidID{0000-0002-7930-110X}}
\authorrunning{S.\ A.\ Alexander}
\institute{The U.S.\ Securities and Exchange Commission
\email{samuelallenalexander@gmail.com}
\url{https://philpeople.org/profiles/samuel-alexander/publications}}

\maketitle

\begin{abstract}
    Fill this in.
\end{abstract}

\section{Introduction}

In this paper, we consider computable agents in a deterministic variant of the reinforcement
learning (RL) framework. Such agents are intended to interact with deterministic RL
environments, where they take actions and receive rewards and observations in
response to those actions. Essentially, such an environment is basically a function $e$
which outputs a reward-observation pair
$\langle r,o\rangle=e(a_0,\ldots,a_n)$ in response to an action-sequence $(a_0,\ldots,a_n)$.

However, there is another type of environment in which these
same RL agents can interact just as well. Essentially, we define an \emph{extended
environment} to be a function $e$ which outputs a reward-observation pair
$\langle r,o\rangle=e((a_0,\ldots,a_n), T)$ in response to an action-sequence
$(a_0,\ldots,a_n)$ along with a Turing machine $T$ which computes an agent.
Intuitively, this should be thought of as follows: when the agent enters the environment,
the environment is made aware of the source-code of the agent, and can use that
source-code when computing what rewards and observations to give to the agent in response
to the agent's actions.

For example, the environment might be a game consisting of a
labyrinth where the agent wanders from
room to room, each room containing a treasure, and most (but not all) rooms containing
a guard. The agent can only see one room at a time, and cannot look ahead into adjacent
rooms. In a room with no guard, the agent can choose to take the treasure, in which
case the agent receives a positive reward.
If the player chooses to take the treasure in a guarded room,
then, by simulating the agent, the guard determines: ``If
I (the guard) were not present in this room, would the agent take the treasure?''
If the answer is ``yes'', then the guard prevents the agent from taking the treasure,
and zaps the agent (which the agent feels as a negative reward, i.e., a punishment).
If the answer is ``no'', then the guard allows the agent to take the treasure (and the
agent is rewarded)\footnote{This environment bears some similarity to Newcomb's
Paradox \cite{nozick1969newcomb}.}.

The above environment would apparently be impossible (or prohibitively expensive)
for a human to play, due to the difficulty of the guards simulating the human player.
But there is no reason why the above game could not be played by an RL agent (the
RL agent's source-code being fed to the game-engine beforehand). Clearly this kind of
extended environment is not the kind of environment the RL agent is intended to be
applied to, however, such environments could be useful on the path to Artificial
General Intelligence (AGI) because they seem to incentivize self-reflection: in order
to perform well in the above example, the agent would evidently need to achieve some
degree of self-awareness in order to figure out the underlying pattern behind the
guards.

We will give examples of some interesting
extended environments of the above kind, designed to incentivize RL agents to
recursively engage in self-reflection in various ways.
We conjecture that traditional RL techniques will not perform well in these extended
environments, because traditional RL techniques do not involve any sort of
self-reflection. Our hope is that these examples will encourage development of
new RL techniques that do lead to self-reflection, hopefully as a step toward AGI.

\section{Preliminaries}

\begin{definition}
(Plays and prompts)
    Suppose $A$ is a finite set (thought of as a set of permissible actions).
    \begin{enumerate}
        \item
        By an \emph{ROA-play from $A$} (or simply an \emph{ROA-prompt} if $A$
        is clear from context), we mean either the empty sequence,
        or else a sequence of the form
        \[
            r_0,o_0,a_0,\ldots,r_k,o_k,a_k
        \]
        where each $r_i\in\mathbb Q$ (thought of as a \emph{reward}),
        each $o_i\in\mathbb N$ (thought of as an \emph{observation}),
        and each $a_k\in A$ (thought of as an \emph{action}).
        \item
        By an \emph{ROA-prompt from $A$} (or simply an \emph{ROA-prompt} if $A$
        is clear from context), we mean a sequence of the form
        $s\frown r\frown o$
        where $s$ is an ROA-play, $\frown$ denotes concatenation,
        $r\in\mathbb Q$ (thought of as a \emph{reward}),
        and $o\in\mathbb N$ (thought of as an \emph{observation}).
    \end{enumerate}
\end{definition}

\begin{definition}
\label{agentandenvironment}
(Agents and environments)
    \begin{enumerate}
    \item An \emph{agent} is a Turing machine which takes as input a finite action-set $A$
    and an ROA-prompt $s$ from $A$, and outputs an action $a\in A$.
    \item An \emph{extended environment} is a pair $\langle A,e\rangle$ such that:
        \begin{enumerate}
            \item $A$ is a finite action-set.
            \item $e$ is a Turing machine.
            \item
            For every agent $T$, for every
            ROA-play $s$ from $A$,
            when $e$ is run on input $\langle T,s\rangle$, $e$ halts on that input
            and outputs a pair $\langle r,o\rangle$ where $r\in\mathbb Q$ (thought of
            as a reward) and $o\in\mathbb N$ (thought of as an observation).
        \end{enumerate}
    \end{enumerate}
\end{definition}

There is a subtle nuance in Definition \ref{agentandenvironment}. Should the agent's
next action depend on the entire history (including prior actions), or only on prior
rewards and observations? One could argue that
the agent's next action needn't depend on its own past actions, since its own past actions
can be inferred from past rewards and observations.
Normally, it would not matter much whether
or not the agent's next action depend on its own past actions\footnote{In
\cite{alexander2019intelligence}, we
formalize agents whose actions depend only on past rewards and observations.
Legg and Hutter give a formalization where the agent's next action does explicitly
depend on its past actions \cite{legg2007universal}.}. In formalizing examples of extended
environments that incentivize self-reflection, we have found it convenient for the agent's
next action to formally depend on past actions. Perhaps this reflects that known
conscious agents (e.g.\ humans)
evidently do \emph{not} carefully re-compute their own
past actions from remembered observations and rewards, but instead, a human maintains
memories of her past actions as well, regardless whether doing so is formally superfluous.


\begin{definition}
    Suppose $T$ is an agent and $E=\langle A,e\rangle$ is an extended environment.
    The \emph{result of $T$ interacting with $E$} is the infinite
    reward-observation-action sequence
    \[r_0,o_0,a_0,r_1,o_1,a_1,\ldots\]
    (each $r_i\in\mathbb Q$, $o_i\in\mathbb N$, and $a\in A$)
    defined inductively as follows.
    \begin{itemize}
        \item $r_0$ and $o_0$ are obtained by computing $e$ on $\langle T,\langle\rangle\rangle$
        where $\langle\rangle$ is the empty ROA-play.
        \item $a_0$ is the output of $T$ on $\langle A, \langle r_0,o_0\rangle\rangle$.
        \item For $i>0$, $r_i$ and $o_i$ are obtained by computing $e$
        on
        \[\langle T,\langle r_0,o_0,a_0,\ldots,r_{i-1},o_{i-1},a_{i-1}\rangle\rangle.\]
        \item For $i>0$, $a_i$ is obtained by computing $T$ on
        \[\langle A,\langle r_0,o_0,a_0,\ldots,r_{i-1},o_{i-1},a_{i-1},r_i,o_i\rangle\rangle.\]
    \end{itemize}
\end{definition}

\section{Examples of Self-reflection-incentivizing Environments}

\begin{example}
\label{rewardagentforignoringrewardsexample}
    (Reward Agent for Ignoring Rewards)
    For each ROA-prompt $p$, let $p^0$ be the ROA-prompt equal to $p$ except that
    all rewards are $0$.
    For any environment $E=\langle A,e\rangle$, we define a new environment
    $E'=\langle A,e'\rangle$ where $e'$ is defined as follows
    (where $T$ is a Turing machine, $p$ is an ROA-prompt, and $a\in A$ is thought of as
    an agent's action in response to $p$):
    \begin{align*}
        e'(T,\langle\rangle) &= e(T,\langle\rangle)\\
        e'(T,p\frown a)
        &= \langle r,o\rangle,
    \end{align*}
    where $o$ is the observation component of
    $e(T,p\frown a)$ (if $e$ does not halt on input $\langle T,p\frown a\rangle$
    (resp.\ $\langle T,\langle\rangle\rangle$)
    then neither does $e'$) and
    \[
        r =
        \begin{cases}
            1 & \mbox{if $a=T(A,p^0)$,}\\
            -1 & \mbox{if $a\not=T(A,p^0)$}
        \end{cases}
    \]
    (if $T$ does not halt on input $\langle A,p^0\rangle$ then $e'$ does not halt on input
    $\langle T,p\frown a\rangle$).
\end{example}

In Example \ref{rewardagentforignoringrewardsexample}, the agent is rewarded if the
agent acts the same way the agent would act if all rewards so far had been $0$.
Otherwise, the agent is punished. Thus, paradoxically, the agent is rewarded for
ignoring rewards. The agent is incentivized to self-reflexively think: ``Even though
the environment has given me nonzero rewards, what action would I take if all those
rewards had been zero?''

\begin{example}
\label{selfinsertionexample}
    (Reward Agent for Self-Inserting)
    Fix a canonical computable bijection
    $o\mapsto \hat o$
    from $\mathbb N$ to $\mathbb Q\times \mathbb N$:
    thus, every observation $o$ encodes a reward-observation pair
    $\hat o = \langle r',o'\rangle$, and every reward-observation pair
    is encoded by some such $o$.
    For any environment $E=\langle A,e\rangle$, we define
    a new environment $E'=\langle A,e'\rangle$ as follows
    (where $T,p,a$ are as in Example \ref{rewardagentforignoringrewardsexample},
    and with similar non-halting caveats
    as Example \ref{rewardagentforignoringrewardsexample}):
    \begin{align*}
        e'(T,\langle\rangle) &= e(T,\langle\rangle)\\
        e'(T,p\frown a) &= \langle r,o\rangle,
    \end{align*}
    where $o$ is such that $\hat o = e(T,p\frown a)$ and
    \[
        r =
        \begin{cases}
            1 & \mbox{if $a=T(A,p')$,}\\
            -1 & \mbox{if $a\not=T(A,p')$}
        \end{cases}
    \]
    where
    $p'$ is the ROA-prompt where each reward-observation pair
    $\ldots,r_i,o_i,\ldots$ is replaced by the reward-observation
    pair $\ldots,\widehat{o_i},\ldots$.
\end{example}

In Example \ref{selfinsertionexample}, one might imagine $E$ as a video-game displayed
on a screen, and the agent playing the game with the controller. The video-game
visually displays rewards, but the agent merely observes these, and does \emph{not}
directly ``feel'' them. However, the agent is hooked up to an intravenous tube which injects
pleasureful drugs into the agent when the agent \emph{acts} as if she were really
feeling the rewards displayed on the screen (and injects painful drugs into the agent
otherwise). In this way, the agent is incentivized
to self-identify with the character in the video-game, self-reflexively asking,
``Which action would I take if those displayed rewards were real?''

\begin{example}
\label{incentivetoincentivizeexample}
    (Incentive to Incentivize)
    Let $R$ be a finite set of rewards (also thought of as actions).
    We define an environment $E=\langle R,e\rangle$ as follows
    (where $T$ is a Turing machine and $r_0,o_0,a_0,\ldots,r_n,o_n,a_n$
    is an ROA-play,
    with similar non-halting caveats as in
    Example \ref{rewardagentforignoringrewardsexample}).
    \begin{align*}
        e(T,\langle\rangle) &= \langle 0, 0\rangle\\
        e(T,r_0,o_0,a_0,\ldots,r_n,o_n,a_n) &= \langle r, 0\rangle,
    \end{align*}
    where
    \[
        r =
        \begin{cases}
            1 & \mbox{if $T(\langle 0,1\rangle,p')=0$},\\
            0 & \mbox{if $T(\langle 0,1\rangle,p')\not=0$}
        \end{cases}
    \]
    where $p'$ is the ROA-prompt $(r'_0,o'_0,a'_0,\ldots,r'_{n+1},o'_{n+1})$
    where $r'_0=0$, each $r'_{i+1}=a_i$,
    each $o'_i=0$, and each
    $a'_i=T(\langle 0,1\rangle, \langle r'_0,o'_0,a'_0,\ldots,r'_i,o'_i\rangle)$.
\end{example}

In Example \ref{incentivetoincentivizeexample}, the agent observes
actions (from action-set $\{0,1\}$), and the agent must act by
choosing rewards for those observed actions.
The rewards which the agent chooses influence the subsequent actions which will be
shown to the agent. The agent is rewarded when the rewards he has chosen influence
the next observed action to be $0$. Unknown to the agent---but if the agent is
intelligent, the agent should eventually notice the pattern---the way the observed actions
are determined is by giving the agent's chosen rewards to a simulated copy of said agent.
Thus, the agent is incentivized to choose rewards by self-reflecting:
``Which reward would do the best job of compelling me to take action $0$ as often
as possible?'' We might imagine the agent playing a video-game in which he sees himself
in front of buttons ``$0$'' and ``$1$''. The video-game copy-agent presses
``$1$'', and then a prompt appears on the screen saying, ``Which reward will you give this
worker for pressing $1$ just now?'' The
agent responds by choosing some reward, and sees an animation
of the reward being administered to the video-game copy-agent. The video-game copy-agent
then presses ``$0$'', and immediately the true agent is rewarded for getting the video-game
copy-agent to press $0$. Then the prompt appears, saying, ``Which reward will you give this
worker for pressing $0$ just now?'' And so on
forever\footnote{Example \ref{incentivetoincentivizeexample} would be more natural if the action-set were
not restricted to a \emph{finite} set of rewards.
That restriction was only made to bring the example in conformance with the usual
finite-action convention.
The example could be modified by allowing
the agent to notate rewards by typing on a keyboard (one action per key-press) or something
similar---``Decision makings often do not happen
at the level of basic operations, but at the level of composed actions, where
there are usually infinite possibilities'' \cite{wang2015assumptions}.
The example is also interesting in that the agent, desiring the clone to take action $0$
as often as possible, is incentivized to choose harsh punishments when the clone takes
action $1$. If punishments are limited to real numbers, then the agent might face a dilemma
similar to one that arises when traditional RL is considered for cancer treatment applications.
An RL surgeon should be punished with an infinitely large negative reward for killing
the patient, but this is impossible if rewards are restricted to real numbers
\cite{wirth2017survey} \cite{zhao2009reinforcement}. Similarly, the agent in Example
\ref{incentivetoincentivizeexample} would probably like to choose infinitely large
punishments, if possible, when its clone from takes action $1$. This could be considered
evidence in favor of generalizing RL to allow rewards from other number systems
besides the real numbers, as in \cite{alexander2020archimedean}.}.

\begin{example}
\label{selfrecognitionexample}
    (Incentivizing Self-recognition)
    Let $s_0,s_1,\ldots$ be an enumeration of all OAR-sequences with award-set
    $\{0,1\}$. We define an environment $E$ as follows.
    Suppose, after initial OAR-sequence
    \[(o_0,a_0,r_0,\ldots,o_{n-1},a_{n-1},r_{n-1},o_n),\]
    in which each $o_i=s_i$,
    the agent takes action $a_n\in\{0,1\}$. If $o_n$ is consistent with the agent
    (meaning that the agent would take the actions in $s_n$ in response to the observations
    and rewards in $s_n$), then give the agent reward $r_n=a_n$.
    Otherwise, give the agent reward $1-a_n$. Let $o_n=s_n$.
\end{example}

In Example \ref{selfrecognitionexample}, the agent is shown various OAR-sequences, and for
each sequence, the agent either pushes button ``Looks like me'' ($1$)
or ``Doesn't look like me'' ($0$). For each OAR-sequence, the agent is rewarded if and
only if the agent correctly guessed whether or not the OAR-sequence is an OAR-sequence
which that agent would produce, given the observations and rewards in the sequence.
Thus, the agent is incentivized to self-reflect in order to ask, ``If I experienced the
observations and rewards in that OAR-sequence, would I act that way?''

The following example is partly motivated by \cite{yampolskiy2012ai}.

\begin{example}
\label{otheraspectsexample}
    (Recognizing other aspects of oneself)
    All the below environments are similar to Example \ref{selfrecognitionexample},
    and we describe them informally to avoid technical details.
    \begin{itemize}
        \item
        (Supervised learning)
        Assume there is a canonical, computable function $f$ which transforms
        each RL agent $A$ into a supervised learning agent $f(A)$. By a \emph{supervised
        learning trial} we mean quadruple $\langle L,T,I,p\rangle$ where $L$ is a finite set
        of labels, $T$ is a sequence of images with labels from $L$ (a \emph{training set}),
        $I$ is an unlabeled image, and $p:L\to \mathbb Q\cap [0,1]$ is a function
        assigning to each label $\ell\in L$ a probability that $\ell$ is the correct label
        for $I$. We define an extended environment as follows.
        The agent $A$ is sytematically shown all supervised learning trials and must
        take action $1$ (``Looks like me'') or $0$ (``Doesn't look like me''), and is
        rewarded or punished accordingly depending whether or not $f(A)$ would
        output $p$ in response to $I$ after being trained with $T$.
        \item
        (Unsupervised learning)
        Assume there is a canonical, computable function $g$ which transforms each RL
        agent $A$ into an unsupervised learning agent $g(A)$.
        By an \emph{unsupervised learning trial} we mean a triple
        $\langle n,D,C\rangle$ where $n$ is a positive integer, $D\subseteq \mathbb Q^n$
        is a finite set of $n$-dimensional points with rational coordinates, and $C$
        is a clustering of $D$.
        We define an extended environment as follows. The agent $A$ is systematically
        shown all unsupervised learning trials and must take action $1$ (``Looks like me'')
        or $0$ (``Doesn't look like me''), and is rewarded or punished accordingly depending
        whether or not $g(A)$ would cluster $D$ into clustering $C$.
        \item
        (The Turing Test)
        Assume there is a canonical, computable function $h$ which transforms each RL
        agent $A$ into an English-speaking chatbot $h(A)$.
        By a \emph{chatbot trial} we mean a sequence of strings of English characters.
        We define an extended environment as follows. The agent $A$ is systematically
        shown all chatbot trials and must take action $1$ (``Looks like me'') or $0$
        (``Doesn't look like me''), and is rewarded or punished accordingly depending
        whether or not even-numbered strings in the chatbot trial are what $h(A)$ would
        say in response to the user saying the odd-numbered strings.
        \item
        (Adversarial sequence prediction) Similar to the above environments, assuming
        a canonical computable function which transforms each RL agent into a predictor
        in the game of adversarial sequence prediction \cite{hibbard2008adversarial}
        \cite{hibbard}.
        \item
        (Mechanical knowing agent) Assume there is a canonical, computable function
        $i$ which transforms each RL agent $A$ into a code $i(A)$ of a computably
        enumerable set of sentences in the language of Epistemic Arithmetic
        \cite{shapiro}; $i(A)$ is thought of as a mechanical knowing agent
        \cite{carlson}. We define an extended environment as follows. The agent $A$
        is systematically shown all sentences in the language of Epistemic Arithmetic,
        with repetition, in such a way that each sentence is shown infinitely often.
        Upon being shown sentence $\phi$ for the $n$th time, the agent must take action
        $1$ (``I know $\phi$ is true'')
        or $0$ (``I'm not sure if $\phi$ is true'')
        and is rewarded if and only if $\phi$ is
        enumerated by $i(A)$ in $\leq n$ steps of computation.
    \end{itemize}
\end{example}

The extended environments in Example \ref{otheraspectsexample} incentivize the agent
to self-reflect, asking itself questions like ``Does that look like how I would
classify that image, given that training set?'' or ``Does that look like how I would
cluster that set of points?'' or ``Does that conversation participant say the same things
I would say?'' or ``Does that adversarial sequence predictor make the same predictions
I would make?'' or ``Does that mechanical knowing agent know the same things I know?''

\bibliographystyle{splncs04}
\bibliography{intro}

\end{document}