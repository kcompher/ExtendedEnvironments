\documentclass[runningheads]{llncs}
\usepackage{amsfonts}
\usepackage{amsmath}

\newtheorem{mytheorem}{Theorem}
\newtheorem{mylemma}[mytheorem]{Lemma}
\newtheorem{myquestion}[mytheorem]{Question}
\newtheorem{myexample}[mytheorem]{Example}
\newtheorem{myproposition}[mytheorem]{Proposition}
\newtheorem{mycorollary}[mytheorem]{Corollary}
\newtheorem{mydefinition}[mytheorem]{Definition}
\newtheorem{myprinciple}[mytheorem]{Principle}

\begin{document}

\title{Extending environments to incentivize self-reflection
in reinforcement learning}
\titlerunning{Extending environments to incentivize self-reflection}

\author{Samuel Allen Alexander\inst{1}\orcidID{0000-0002-7930-110X}}
\authorrunning{S.\ A.\ Alexander}
\institute{The U.S.\ Securities and Exchange Commission
\email{samuelallenalexander@gmail.com}
\url{https://philpeople.org/profiles/samuel-alexander/publications}}

\maketitle

\begin{abstract}
    We consider an extended notion
    of reinforcement learning environment, in which the environment is able
    to simulate the agent. We give
    examples of some such extended environments which seem to incentivize
    various different types of self-reflection or self-awareness.
    These environments are not particularly useful in themselves, but we
    hope they might serve to guide the development of self-aware reinforcement
    learning agents, as well as to help measure the degree to which existing
    reinforcement learning agents are or are not self-aware. We also speculate
    about interesting subjective conscious experiences which
    might be incentivized in self-aware reinforcement learning agents
    placed within these extended environments.
\end{abstract}

\section{Introduction}

\begin{quote}
    ``It is difficult to get a man to understand something, when his salary depends
    upon his not understanding it!''---Upton Sinclair
\end{quote}

What would you do if you were being paid \$100 per hour to act as if you were being
paid \$1000 per hour? What if, in order to receive that \$100 every hour, you were
required to act \emph{exactly} as you would act if you were really being paid
\$1000 that hour---and if you did not so act, then instead of being
paid \$100 that hour, you were penalized \$100 instead?
This thought experiment would be hard to perform, because it would be hard for your
employer to know how you would act if you really were being paid \$1000 per hour.
But if your employer could simulate a perfect copy of you, then your employer could
perform the experiment perfectly. Over a long enough time, would those \$100 rewards
and penalties eventually cause you to believe you were really being paid
\$1000 per hour?

In this paper, we consider a deterministic variant of the reinforcement
learning (RL) framework. For sake of simplicity, we restrict our attention
to deterministic environments and deterministic agents, but the basic idea
would easily adapt
to non-deterministic RL. RL agents are intended to interact with deterministic RL
environments, where they take actions and receive rewards and observations in
response to those actions.

Traditionally, a deterministic RL environment is essentially a function
$e$ which outputs a reward-observation pair
$\langle r,o\rangle=e(r_0,o_0,a_0,\ldots,r_n,o_n,a_n)$ in response to a
reward-observation-action sequence $(r_0,o_0,a_0,\ldots,r_n,o_n,a_n)$.
However, there is another type of environment in which
computable RL agents can interact just as well. Essentially, we define an \emph{extended
environment} to be a function $e$ which outputs a reward-observation pair
\[
    \langle r,o\rangle=e(T,\langle r_0,o_0,a_0,\ldots,r_n,o_n,a_n \rangle)
\]
in response
to a reward-observation-action sequence
along with a Turing machine $T$ which computes the agent.
Intuitively, this should be thought of as follows: when the agent enters the environment,
the environment is made aware of the agent's source-code, and can use that
source-code to simulate the agent when computing rewards and observations.

For example, imagine a game\footnote{This game bears some similarity to Newcomb's
Paradox \cite{nozick1969newcomb}.} consisting of a
labyrinth where the player wanders from
room to room, each room containing a treasure, and most (but not all) rooms containing
a guard.
\begin{itemize}
    \item
    The player can only see one room at a time, and cannot look ahead into adjacent
    rooms, nor visit the same room twice.
    \item
    In a room with no guard, the player can take the treasure, yielding a reward.
    \item
    If the player chooses to take the treasure in a guarded room,
    then, by simulating the player, the guard determines: ``If
    this room had been unguarded, would the player have taken the treasure?''
    If so, the guard blocks the player and zaps them (yielding a negative reward).
    Otherwise, the guard allows the player to take the treasure (and the player is rewarded).
\end{itemize}

The above game would apparently be impossible (or prohibitively expensive)
for a human to play, due to the difficulty of simulating a human player.
But there is no reason why the above game could not be played by an RL agent (the
agent's source-code being given to the game-engine beforehand). Clearly this kind of
extended environment is not the sort of environment RL agents are traditionally
intended to interact with\footnote{Such environments might, however,
accidentally arise if both environment
and agent are implemented on the same machine and the environment is managed by an AI
sophisticated enough to exploit unintended informational side channels, as in
\cite{yampolskiy2012leakproofing}.}. However, such environments could be
useful on the path to Artificial
General Intelligence (AGI) because they seem to incentivize self-awareness.

One might try to imitate an extended environment with a traditional environment by
backtracking---rewinding the environment itself to a prior state after seeing how the
agent performs along one path, and then sending the agent along a second path.
But the agent itself would retain memory of the first path, and the agent's decisions
along the second path might be altered by said memories, so the result would not be
the same as immediately sending the agent along the second path while secretly simulating
the agent to determine what the agent would do if sent along the first path.

We will give examples of interesting
extended environments of the above kind, designed to incentivize RL agents to
recursively engage in self-awareness in various ways.
We conjecture that traditional RL agents would perform poorly in these extended
environments, because traditional RL techniques do not involve any sort of
self-awareness. We hope these examples will facilitate
new RL techniques that do involve self-awareness, hopefully as a step toward AGI.

\section{Preliminaries}

\begin{definition}
(Plays and prompts)
    \begin{enumerate}
        \item
        By an \emph{ROA-play}, we mean either the empty sequence $\langle\rangle$,
        or else a sequence of the form
        \[
            \langle r_0,o_0,a_0,\ldots,r_k,o_k,a_k\rangle
        \]
        where each $r_i\in\mathbb Q$ (thought of as a \emph{reward}),
        each $o_i\in\mathbb N$ (thought of as an \emph{observation}),
        and each $a_k\in \mathbb N$ (thought of as an \emph{action}).
        \item
        By an \emph{ROA-prompt}, we mean a sequence of the form
        $s\frown r\frown o$
        where $s$ is an ROA-play, $\frown$ denotes concatenation,
        $r\in\mathbb Q$ (thought of as a \emph{reward}),
        and $o\in\mathbb N$ (thought of as an \emph{observation}).
    \end{enumerate}
\end{definition}

\begin{lemma}
\label{roaplaydecompositionlemma}
    If $s$ is an ROA-play, then either $s=\langle\rangle$, or else
    $s=p\frown a$ for some ROA-prompt $p$ and action $a\in\mathbb N$.
\end{lemma}

\begin{proof}
    If $s\not=\langle\rangle$ then we can write
    $s=\langle r_0,o_0,a_0,\ldots,r_k,o_k,a_k \rangle$.

    \textbf{Case 1:}
    $k=0$. By definition, $\langle\rangle$ is an ROA-play,
    therefore $p=\langle\rangle\frown r_0\frown o_0$ is an ROA-prompt,
    and $s=p\frown a_0$.

    \textbf{Case 2:} $k>0$.
    Then $p'=\langle r_0,o_0,a_0,\ldots,r_{k-1},o_{k-1},a_{k-1}\rangle$ is an ROA-play,
    therefore $p=p'\frown r_k\frown o_k$ is an ROA-prompt,
    and $s=p\frown a_k$.
    \qed
\end{proof}

\begin{definition}
\label{agentandenvironment}
(Agents and environments)
    \begin{enumerate}
    \item An \emph{agent} is a Turing machine which
    halts whenever it is run on an ROA-prompt, outputting
    an action $a\in\mathbb N$.
    \item An \emph{extended environment} is a Turing machine $e$ such that:
        \begin{itemize}
            \item
            For every agent $T$, for every
            ROA-play $s$,
            when $e$ is run on input $\langle T,s\rangle$, $e$ halts on that input,
            outputting a pair $\langle r,o\rangle$ where $r\in\mathbb Q$ (thought of
            as a reward) and $o\in\mathbb N$ (thought of as an observation).
        \end{itemize}
    \end{enumerate}
\end{definition}

There is a subtle nuance in Definition \ref{agentandenvironment}. Should the agent's
next action depend on the entire history (including prior actions), or only on prior
rewards and observations? One could argue that
the agent's next action needn't depend on its own past actions, since its own past actions
can be inferred from past rewards and observations.
Normally, it would not matter much whether
or not the agent's next action depend on its own past actions\footnote{In
\cite{alexander2019intelligence}, we
formalize agents whose actions depend only on past rewards and observations.
Legg and Hutter give a formalization where the agent's next action does explicitly
depend on its past actions \cite{legg2007universal}.}. In formalizing examples of extended
environments that incentivize self-reflection, we have found it convenient for the agent's
next action to formally depend on past actions. Perhaps this reflects that known
conscious agents (e.g.\ humans)
evidently do \emph{not} carefully re-compute their own
past actions from remembered observations and rewards, but instead, a human maintains
memories of her past actions as well, regardless whether doing so is formally superfluous.


\begin{definition}
\label{interactiondefn}
    Suppose $T$ is an agent and $e$ is an extended environment.
    The \emph{result of $T$ interacting with $e$} is the infinite
    reward-observation-action sequence
    \[\langle r_0,o_0,a_0,r_1,o_1,a_1,\ldots\rangle\]
    (each $r_i\in\mathbb Q$, $o_i,a_i\in\mathbb N$)
    defined inductively as follows.
    \begin{itemize}
        \item $r_0$ and $o_0$ are obtained by computing $e$ on
        $\langle T,\langle\rangle\rangle$.
        \item $a_0$ is the output of $T$ on $\langle r_0,o_0\rangle$.
        \item For $i>0$, $r_i$ and $o_i$ are obtained by computing $e$
        on
        \[\langle T,\langle r_0,o_0,a_0,\ldots,r_{i-1},o_{i-1},a_{i-1}\rangle\rangle.\]
        \item For $i>0$, $a_i$ is obtained by computing $T$ on
        \[\langle r_0,o_0,a_0,\ldots,r_{i-1},o_{i-1},a_{i-1},r_i,o_i\rangle.\]
    \end{itemize}
\end{definition}

\begin{lemma}
    For any agent $T$ and extended environment $e$, the result of $T$ interacting
    with $e$ (Definition \ref{interactiondefn}) is defined (all of the computations
    in question halt with the necessary outputs).
\end{lemma}

\begin{proof}
    By a simultaneous induction:
    \begin{itemize}
        \item
        Each $r_i$ and $o_i$ are defined (and $r_i\in\mathbb Q$
        and $o_i\in\mathbb N$) because, by induction,
        $\langle r_0,o_0,a_0,\ldots,r_{i-1},o_{i-1},a_{i-1}\rangle$
        is defined and is an ROA-play (because, inductively,
        each $r_j\in\mathbb Q$, $o_j\in\mathbb N$ and $a_j\in\mathbb N$
        for all $j<i$) and thus
        $r_i$ and $o_i$ are defined with the correct form by
        Definition \ref{agentandenvironment} (part 2).
        \item
        Each $a_i$ is defined (and $a_i\in\mathbb N$) because, by induction,
        $\langle r_0,o_0,a_0,\ldots,r_i,o_i\rangle$
        is defined and is an ROA-prompt (similar to the above) and thus
        $a_i$ is defined with the correct form by Definition
        \ref{agentandenvironment} (part 1).
    \end{itemize}
    \qed
\end{proof}

One important implication of extended environments is that they further divide
the (already divided) ways of measuring intelligence of RL agents. Intelligence
measures
\cite{alexander2019intelligence} \cite{hernandez} \cite{legg2007universal}
which aggregate performance over traditional environments only measure
an agent's intelligence over those environments. The same measures could easily
be extended to also take extended environments into account, perhaps providing
measures which better capture agents' self-awareness and self-reflection abilities.

\section{Examples of Self-awareness-incentivizing Environments}
\label{basicexamplessection}

In this section, we give simple examples of interesting extended environments which seem
to incentivize various forms of self-awareness. We are inspired by various libraries of
traditional RL environments \cite{bellemare2013arcade}
\cite{beyret2019animal} \cite{brockman2016openai} \cite{chollet2019measure}
\cite{cobbe2020leveraging}. All the environments in this section have a certain special
form: they always output rewards from $\{1,-1\}$ and they always output observation $0$.
This is intentional: in the next section, this uniformity will allow all these examples
to be generalized.

\begin{example}
\label{rewardagentforignoringrewardsexample}
    (Reward Agent for Ignoring Rewards)
    For each ROA-prompt $p$, let $p^0$ be the ROA-prompt equal to $p$ except that
    all rewards are $0$.
    We define an extended environment $e$ as follows
    (where $T$ is a Turing machine, $p$ is an ROA-prompt, and $a\in \mathbb N$ is thought of as
    the agent's action in response to $p$):
    \begin{align*}
        e(T,\langle\rangle) &= \langle 0,0\rangle\\
        e(T,p\frown a)
        &= \langle r,0\rangle,
    \end{align*}
    where
    \[
        r =
        \begin{cases}
            1 & \mbox{if $a=T(p^0)$,}\\
            -1 & \mbox{if $a\not=T(p^0)$}
        \end{cases}
    \]
    (if $T$ does not halt on $p^0$ then $e$ does not halt on
    $\langle T,p\frown a\rangle$).
\end{example}

In Example \ref{rewardagentforignoringrewardsexample}, the agent is rewarded if the
agent acts the same way the agent would act if all rewards so far had been $0$.
Otherwise, the agent is punished. Thus, paradoxically, the agent is rewarded for
ignoring rewards. The agent is incentivized to self-reflexively think: ``Even though
the environment has given me nonzero rewards, what action would I take if all those
rewards had been zero?''

\begin{lemma}
\label{example1workslemma}
    Example \ref{rewardagentforignoringrewardsexample} really does define an
    extended environment.
\end{lemma}

\begin{proof}
    Let $e$ be as in
    Example \ref{rewardagentforignoringrewardsexample}.
    We must show $e$ is an extended environment (Definition \ref{agentandenvironment}
    part 2). We must show that for each agent $T$ and ROA-play $s$,
    $e$ halts on $\langle T,s\rangle$ and outputs a pair $\langle r,o\rangle$
    such that $r\in\mathbb Q$ and $o\in\mathbb N$.

    \textbf{Case 1:} $s=\langle\rangle$. Then $e(T,s)$ halts with output
    $\langle 0,0\rangle$, so $r=0\in\mathbb Q$ and $o=0\in\mathbb N$.

    \textbf{Case 2:} $s\not=\langle\rangle$. By Lemma \ref{roaplaydecompositionlemma},
    $s=p\frown a$ for some ROA-prompt $p$ and action $a\in\mathbb N$.
    Since $p$ is an ROA-prompt, clearly $p^0$ is also an ROA-prompt, therefore
    since $T$ is an agent, Definition \ref{agentandenvironment} (part 1)
    guarantees $T(p^0)$ is defined and is in $\mathbb N$. It follows that the reward
    $r$ in Definition \ref{rewardagentforignoringrewardsexample} is defined and is in
    $\mathbb Q$. And certainly the observation $0$ is in $\mathbb N$.
    So $e(T,s)$ outputs a pair $\langle r,o\rangle$ meeting the necessary
    requirements.
    \qed
\end{proof}

For future examples, we will suppress the corresponding lemmas like
Lemma \ref{example1workslemma} which say that those examples really work.

Example \ref{rewardagentforignoringrewardsexample} is profound because it illustrates how,
in an extended environment, it is possible to give one sequence of rewards
in order to incentivize the agent to act as if a different sequence of rewards was given.
Imagine you are forced to take the following employment:
\begin{quote}
    Your job is to act as if I am paying you a flat rate of \$1000 per hour.
    Every hour that you act as if
    I am paying you \$1000 per hour, I will pay you \$100. But every hour that
    you do \emph{not} act as if I am paying you \$1000 per hour,
    I will deduct \$100 from your paycheck.
\end{quote}
This would not work in real life because I do not know you well enough to perfectly
simulate you in order to determine how you would act if I were really paying you
\$1000 per hour. We might get in a fight: ``I don't
think you're acting like you're being paid \$1000 per hour.''
``You're wrong, I \emph{am} being paid \$1000 per
hour.'' But if I could simulate you perfectly, I could indeed hire you in this way.
Assuming you need that \$100, you would be motivated to sincerely act as
if you were being paid \$1000 per hour. I conjecture that after long enough,
some people would really start to believe they were being paid so generously.

\begin{example}
\label{falsememoryexample}
    (False Memories)
    Suppose $p_0$ is any ROA-play. We define an extended environment
    $e$ as follows
    (with similar non-halting caveats as
    Example \ref{rewardagentforignoringrewardsexample}):
    \begin{align*}
        e(T,\langle\rangle) &= \langle 0,0\rangle,\\
        e(T,p\frown a) &= \langle r,0\rangle,
    \end{align*}
    where
    \[
        r=
        \begin{cases}
            1 &\mbox{if $a=T(p_0\frown p)$,}\\
            -1 &\mbox{if $a\not=T(p_0\frown p)$}.
        \end{cases}
    \]
\end{example}

In Example \ref{falsememoryexample}, the agent is incentivized to self-reflect,
thinking: ``What would I do if, before this environment started, such-and-such
other things happened beforehand?'' If a stranger hired you to act like their lifelong
friend, you probably wouldn't be sincere in your acting. But if said stranger could
perfectly simulate you in order to base your pay on your acting like you \emph{really
would} act if you were their lifelong friend, then you would be incentivized to
find some way to make yourself remember being their lifelong friend, despite never
having been.

Henceforth, we will not explicitly mention the non-halting caveats in the following
examples.

\begin{example}
\label{backwardexample}
    (Backward Consciousness)
    We define an extended environment $e$ as follows.
    \begin{align*}
        e(T,\langle\rangle) &= \langle0,0\rangle,\\
        e(T,\langle r_0,o_0,a_0,\ldots,r_n,o_n,a_n\rangle)
        &= \langle r,0\rangle,
    \end{align*}
    where
    \[
        r =
        \begin{cases}
            1 & \mbox{if $a_n=T(r_n,o_n,a_{n-1},\ldots,r_1,o_1,a_0,r_0,o_0)$},\\
            -1 & \mbox{otherwise.}
        \end{cases}
    \]
\end{example}

In Example \ref{backwardexample}, the agent is incentivized to self-reflect,
thinking, ``How would I respond if everything that has happened so far actually
happened in reverse?'' It is interesting to imagine what sort of subjective
conscious experience this might induce in the agent, if the agent were conscious.
Would the incentives eventually brainwash the agent into perceiving itself
moving backward through time?


\begin{example}
\label{dejavuexample}
    (D\'{e}j\`{a} Vu)
    We define an environment $e$ as follows:
    \begin{align*}
        e(T,\langle\rangle) &= \langle 0,0\rangle\\
        e(T,p\frown a) &= \langle r,0\rangle\\
    \end{align*}
    where
    \[
        r =
        \begin{cases}
            1 & \mbox{if $T(p\frown a\frown p)=a$},\\
            -1 & \mbox{if $T(p\frown a\frown p)\not=a$}.
        \end{cases}
    \]
\end{example}

In Example \ref{dejavuexample},
the agent is incentivized to self-reflect and ask: ``Which action
would I take in order to ensure that I would take that same action if everything
which has happened so far were to repeat itself verbatim?''

\begin{example}
\label{incentivetoincentivizeexample}
    (Incentive to Incentivize)
    We define an environment $e$ as follows:
    \begin{align*}
        e(T,\langle\rangle) &= \langle 0, 0\rangle\\
        e(T,\langle r_0,o_0,a_0,\ldots,r_n,o_n,a_n\rangle) &= \langle r, 0\rangle,
    \end{align*}
    where
    \[
        r =
        \begin{cases}
            1 & \mbox{if $T(p')=0$},\\
            -1 & \mbox{if $T(p')\not=0$}
        \end{cases}
    \]
    where $p'$ is the ROA-prompt $(r'_0,o'_0,a'_0,\ldots,r'_{n+1},o'_{n+1})$
    where $r'_0=0$, each $r'_{i+1}=a_i$,
    each $o'_i=0$, and each
    $a'_i=T(r'_0,o'_0,a'_0,\ldots,r'_i,o'_i)$.
\end{example}

In Example \ref{incentivetoincentivizeexample}, the agent is tasked with choosing
rewards in such a way that if those rewards were fed to a simulated copy of the agent,
then the simulated copy would take action $0$.
Thus, the agent is incentivized to choose rewards by self-reflecting:
``Which rewards would do the best job of compelling me to take action $0$ as often
as possible?'' We might imagine the agent playing a video-game in which he sees himself
in front of a keyboard. The video-game copy-agent types
``$100$'', the true agent is punished because $100\not=0$,
and then a message appears on the screen saying, ``Which reward will you give this
worker for typing $100$ just now?'' The
agent responds by choosing some reward, and sees an animation
of the reward being given to the video-game copy-agent. The video-game copy-agent
then types ``$0$'', and immediately the true agent is rewarded for getting the video-game
copy-agent to type $0$. Then a message appears, saying, ``Which reward will you give this
worker for typing $0$ just now?'' And so on
forever\footnote{Example \ref{incentivetoincentivizeexample}
is interesting in that the agent, desiring the clone to take action $0$
as often as possible, is incentivized to choose large rewards when the clone takes
action $0$.
If rewards are limited to $\mathbb Q$, then the agent faces a dilemma
similar to one in RL cancer treatment applications.
An RL doctor should be punished with an infinitely large negative reward for killing
a patient, but this is impossible if rewards are restricted to rational
(or even real) numbers
\cite{wirth2017survey} \cite{zhao2009reinforcement}. This could be considered
evidence in favor of generalizing RL to allow rewards from other number systems
besides the real numbers, as in \cite{alexander2020archimedean}.}.

Many other interesting examples could be given. For example, an extended environment
could reward agents based on how many steps (or how much memory)
they use to compute each action\footnote{In some sense, by giving the environment
access to the agent's source-code, we allow the environment to reflect the agent's
own internal signals. Thus, extended environments seem to generalize the idea of
agents modified to manually predict their own internal signals, as in
\cite{sherstan2016introspective}.};
we do not intend the list of examples in this paper to be exhaustive.
We will now discuss how all the examples in this section can be generalized.

\section{New extended environments from old}

In this section, we will discuss ways of obtaining new environments from old.
These procedures could be iterated (just like layers of a neural network),
in order to build up environments incentivizing deeper and deeper nested self-awareness.
First, we will generalize the examples from Section \ref{basicexamplessection}.

\begin{definition}
    (Handicaps)
    \begin{enumerate}
        \item
        An extended environment is \emph{merciful} if it never outputs negative rewards.
        \item
        By a \emph{handicap}, we mean an extended environment which always outputs $0$ as
        observation and always outputs either $1$ or $-1$ as reward.
        \item
        If $e$ is a merciful extended environment and $h$ is a handicap,
        we define a new environment $e*h$ as follows:
        \[
            (e*h)(T,p) =
            \begin{cases}
                \langle r_e, o_e\rangle &\mbox{if $r_h=1$},\\
                \langle -1, o_e\rangle &\mbox{if $r_h=-1$},
            \end{cases}
        \]
        where $e(T,p)=\langle r_e,o_e\rangle$ and $h(T,p)=\langle r_h,0\rangle$.
    \end{enumerate}
\end{definition}

Intuitively, $e*h$ is just like $e$ except that $h$ imposes an additional constraint
on the agent. Any time the agent violates that constraint, the agent is punished,
and forfeits any reward that would otherwise have been won from $e$. Aside from the
pain caused by $h$, the agent otherwise observes $e$ unaltered (that is, the observations
from $e$ are not changed). We require $e$ to be merciful in order that large negative
rewards from $e$ do not confuse the intended incentive, for if the agent could avoid
a reward even smaller than $-1$ by intentionally using the handicap, then it would not
be much of a handicap in the everyday meaning of that word. The requirement that $e$ be
merciful could be replaced by various weaker requirements if we revised the RL framework
to allow infinitary rewards, as in \cite{alexander2020archimedean}.

\begin{example}
    Each example in Section \ref{basicexamplessection} is a handicap.
    For any merciful extended environment $e$, each example $h$ in
    Section \ref{basicexamplessection} can be applied as a handicap, yielding a
    version $e'=e*h$ of $e$ modified to incentivize the corresponding type of self-awareness.
    \begin{itemize}
        \item
        Modifying $e$ using Example \ref{rewardagentforignoringrewardsexample}
        (``Reward Agent for Ignoring Rewards'')
        yields a version of $e$ where the agent is penalized for taking actions in
        response to nonzero rewards. The agent is incentivized to strategically
        take non-bored actions (for which it receives small penalties) in order to
        put itself in a position where its next bored action coincidentally is an
        action which wins a large enough reward from $e$ to make up for said
        penalties.
        \item
        Modifying $e$ using Example \ref{falsememoryexample}
        (``False Memories'') yields a version of $e$ where the agent is penalized
        whenever it acts inconsistently with a fictitious history. The agent is
        incentivized to strategically choose when to act so inconsistently so that
        a later action, consistent with said false history, happens to win a large
        reward from $e$. For example, the agent hired to act as a lifelong friend
        might strategically choose to abandon its employer (an action inconsistent
        with lifelong friendship) during a time of small need, so as to be able to
        swoop in and save the day (an action consistent with lifelong friendship)
        during a time of great need.
        \item
        Modifying $e$ using Example \ref{backwardexample}
        (``Backward Consciousness'') would yield a version of $e$
        where the agent must act as if time is reversed, or else suffer punishment.
        The agent can strategically choose to accept some punishment, acting other
        than it would act if time really were reversed, in order to get into a state
        where subsequently acting as if time is reversed will yield more reward.
        \item
        Modifying $e$ using Example \ref{dejavuexample} (``D\'{e}j\`{a} Vu'')
        would yield a version of $e$ where the agent is incentivized
        to act as if
        everything (including said action) had all happened before.
        The agent can strategically choose to not so act (thus suffering some
        pain) in order to get to a state where it is easier to so act and
        to gain rewards from $e$ by so acting.
        \item
        Modifying $e$ using Example \ref{incentivetoincentivizeexample}
        (``Incentive to Incentivize'') would yield an environment in which
        the agent watches a copy of itself interacting with $e$, and has to
        guide that copy by choosing rewards to give to it, in such a
        way as to incentivize the copy to take prescribed actions. When the copy
        takes the prescribed action, the true agent enjoys whatever rewards
        $e$ would give for that action. The agent can strategically guide the
        copy to take non-prescribed actions (this will cause the agent to suffer
        pain), so as to get the copy into a position where subsequently taking
        prescribed actions will yield more reward.
    \end{itemize}
\end{example}

So far, in all our examples, observations have either been $0$ or else have been
imported without modification from an initial environment. In the next example,
we will finally make some nontrivial usage of observations. Using carefully chosen
observations, we will incentivize the agent, who might be thought of as
controlling a character on a video-game screen,
to ``suspend disbelief'' and act as if actually being that character.

\begin{example}
\label{selfinsertionexample}
    (Reward Agent for Self-Inserting)
    Fix a canonical computable bijection
    $o\mapsto \hat o$
    from $\mathbb N$ to $\mathbb Q\times \mathbb N$:
    thus, every observation $o$ encodes a reward-observation pair
    $\hat o = \langle r',o'\rangle$, and every reward-observation pair
    is encoded by some such $o$.
    For any environment $e$, we define
    a new environment $e'$ as follows:
    \begin{align*}
        e'(T,\langle\rangle) &= e(T,\langle\rangle)\\
        e'(T,p\frown a) &= \langle r,o\rangle,
    \end{align*}
    where $o$ is such that $\hat o = e(T,p\frown a)$ and
    \[
        r =
        \begin{cases}
            1 & \mbox{if $a=T(p')$,}\\
            -1 & \mbox{if $a\not=T(p')$}
        \end{cases}
    \]
    where
    $p'$ is the ROA-prompt obtained from $p$
    by replacing each reward-observation pair
    $\ldots,r_i,o_i,\ldots$ by the reward-observation
    pair $\ldots,\widehat{o_i},\ldots$.
\end{example}

In Example \ref{selfinsertionexample}, one might imagine $e'$ as a room containing nothing
but an arcade game $e$. There is nothing for the agent in the room to do
except play this arcade game.
When played, the arcade game
visually displays rewards, but the agent merely observes them, and does not
``feel'' them. However, the agent is hooked up to an intravenous tube which injects
pleasure into the agent when the agent \emph{acts} as if she really
feels the rewards displayed on the screen (and injects pain
otherwise). In this way, the agent is incentivized
to self-identify with the protagonist in the video-game, self-reflexively asking,
``Which action would I take if those displayed rewards were real?''


\section{Some more ambitious examples}

\subsection{Playing in the mirror}

\begin{quote}
    ``I may add that when a few days under nine months old he associated his own name with
    his image in the looking-glass, and when called by name would turn towards the glass
    even when at some distance from it.''---Charles Darwin \cite{darwin1877biographical}
\end{quote}

It has been suggested that the act of recognizing oneself in the mirror is linked
to the development of certain parts of the human psychology
\cite{lacan}. Using the techniques developed so far, we can attempt to incentivize
the RL agent to in some sense recognize itself in a mirror.

\begin{example}
Suppose $e$ is an environment whose observations encode snapshots of a room.
Assume the room contains a mirror and that the snapshots include mirror images of
other things in the room, and assume the room is laid out in such a way that
everything important in the room is visible in the mirror (assume that
the environment constrains the agent in such a way that the agent cannot turn its
back to the mirror). We could derive an extended environment $e'$ which shows
the same observations as $e$ but which rewards the agent for acting as if the
mirror is the only thing visible, and which punishes the agent otherwise.
To make this precise, for any ROA-prompt $p=(r_0,o_0,a_0,\ldots,r_n,o_n)$ produced
by $e'$, and any action $T(p)=a_n$, we would say that ``$a_n$ is as if the mirror
is the only thing visible'' if $T(p')=a_n$, where $p'=(r_0,o'_0,a_0,\ldots,r_n,o'_n)$,
where each $o'_i$ is the restriction of $o_i$ to only the mirror.
\end{example}

To make this even more elaborate, the $o'_i$ in the above example could be further
modified by adding an image of the agent's ``body'' into the mirror. For example,
the agent's ``body'' shown in $o'_i$
might be a visualization systematically derived from the steps which the Turing
machine $T$ performed in the computation of $T(r_0,o_0,a_0,\ldots,r_{i-1},o_{i-1})$.
These steps would not be available to a traditional RL environment, but they are
available to an extended environment because of the inclusion of $T$ itself as an
argument passed to the extended environment.

\subsection{Binocular vision}

Humans seem to consciously perceive a three-dimensional model of their surrounding
world, even though the raw data which we actually receive consists of two two-dimensional
image-feeds (one for each eye). The following example is intended to incentivize an RL
agent to learn to perceive the world through binocular vision like a human.

\begin{example}
\label{binocularexample}
    Suppose $V$ is a video game intended to be played on a virtual-reality headset,
    so at any moment during the game, $V$ produces two snapshots, one for the player's
    left eye, one for the player's right eye. Assume the player is constrained in $V$
    so as never to be able to put their eyes into weird configurations (such as
    the weird configurations in
    \cite{gallagher2020third}): thus, at any moment, the two snapshots $s_1,s_2$
    which $V$ is
    displaying to the player are equivalent to a single 3-D matrix encoding
    the model $m(s_1,s_2)$ which the player is intended to perceive (with cells blanked
    out where the player's vision is obstructed by obstacles). Let $e$ be the extended
    environment whose observations encode pairs $\langle s_1,s_2\rangle$ of snapshots
    displayed by $V$ in response to the player pressing keys encoded by the agent's
    actions. In response to an ROA-play $(r_0,o_0,a_0,\ldots,r_n,o_n,a_n)$ (where
    each $o_i$ encodes $\langle s^i_1,s^i_2\rangle$), let $r_{n+1}=1$ if
    $a_n=T(r_0,o'_0,a_0,\ldots,r_n,o'_n)$, where each $o'_i$ encodes
    $m(s^i_1,s^i_2)$, otherwise let $r_{n+1}=-1$.
\end{example}

In Example \ref{binocularexample}, upon being presented a sequence of pairs of snapshots,
the agent is incentivized to self-reflectively ask, ``Which action would I take if instead
of observing those 2-D snapshot-pairs, I observed equivalent 3-D snapshot-singletons?''

\subsection{Nature and Nurture}

\begin{quote}
    ``If only one soul was created, and all human souls are descended from it,
    who can say that he did not sin when Adam sinned?''---Augustine \cite{augustine1993free}
\end{quote}

How much of our personality is unique to us, and how much is common to all humans?
Would I perform the same actions as you if I were exposed to the exact same stimuli as
you all my life? Probably not, because our bodies are different; but in what sense are
we our bodies, and in what sense are our bodies part of our environment?
The following example is motivated by contemplating the possibility that maybe we all
run the same software on some deep level.

\begin{example}
\label{cryingbabyexample}
    (The Crying Baby)
    We define an extended environment $e$ as follows.
    \begin{itemize}
        \item
        Considered as actions taken by an adult
        (and also as observations seen by a baby), let $0$ denote ``feed the baby''
        and let all naturals $>0$ denote ``don't feed the baby''.
        \item
        Considered as actions taken by a baby
        (and also as observations seen by an adult), let $0$ denote ``laugh'' and let
        all naturals $>0$ denote ``cry''.
        \item
        We define a function $s$, which stands for \emph{satiation}, defined on
        ROA-plays, by $s(p)=100+25f(p)-\mbox{len}(p)$ where $f(p)$ is the number of
        times that the action ``feed the baby'' is taken in $p$,
        and $\mbox{len}(p)$ is the length of $p$.
        \item
        Let $e(T,\langle\rangle)=\langle 1,\mbox{``laugh''}\rangle$.
        \item
        For each ROA-play $\langle r_0,o_0,a_0,\ldots,r_n,o_n,a_n\rangle$,
        let
        \[
            e(T,\langle r_0,o_0,a_0,\ldots,r_n,o_n,a_n\rangle)
            =
            \langle r,o\rangle
        \]
        where $r$ and $o$ are defined as follows.
        For each $i=0,\ldots,n$, define
        \begin{align*}
            r'_i &=
                \begin{cases}
                    1 &
                    \mbox{if
                    $50\leq s(\langle r_0,o_0,a_0,\ldots,r_i,o_i,a_i\rangle)\leq 200$,}\\
                    -1 & \mbox{otherwise,}
                \end{cases}\\
            o'_i &= a_i\\
            a'_i &= T(\langle r'_0,o'_0,a'_0,\ldots,r'_i,o'_i\rangle).
        \end{align*}
        Define $o=a'_n$ and
        \[
            r =
            \begin{cases}
                1 & \mbox{if $a'_n=\mbox{``laugh''}$,}\\
                -1 &\mbox{if $a'_n=\mbox{``cry''}$.}
            \end{cases}
        \]
    \end{itemize}
\end{example}

In Example \ref{cryingbabyexample}, rather than mentally self-reflecting,
the agent is physically self-reflected into the form of a newborn baby
who has inherited the agent's own source-code. Despite having the same
internal source-code $T$ (``nature''), the agent and the baby behave
differently because they perceive the environment differently (``nurture'').
The baby's objective is to maintain satiation within a satisfying range,
whereas the agent's objective is to pacify the baby.

Using the same basic idea, one could easily extend Example \ref{cryingbabyexample}
into an example modelling an entire society of interacting people of different
types, all sharing the same internal ``nature'' ($T$).
If the simulation were sophisticated enough, it would include different people
receiving different educations, ``nurturing'' them to have different
personalities despite their common nature\footnote{To quote Plato:
``Education, then, is a matter of correctly disciplined feelings of pleasure
and pain'' \cite{platolaws}.}.

\section{Examples involving self-recognition}

\begin{example}
\label{selfrecognitionexample}
    (Incentivizing Self-recognition)
    Let $p_0,p_1,\ldots$ be a canonical computable enumeration of all non-empty ROA-plays.
    We define an environment $e$ as follows:
    \begin{align*}
        e(T,\langle\rangle) &= \langle 0, p_0\rangle\\
        e(T,p\frown a) &= \langle r, p_{n}\rangle
    \end{align*}
    where $n=\frac{\mbox{len}(p\frown a)}{3}$ is the number of actions in $p\frown a$ and where
    \[
        r =
        \begin{cases}
            1 &\mbox{if $a>0$ and $a'=T(p')$,}\\
            1 &\mbox{if $a=0$ and $a'\not=T(p')$,}\\
            0 &\mbox{otherwise}
        \end{cases}
    \]
    where $p_{n-1}=p'\frown a'$.
\end{example}

In Example \ref{selfrecognitionexample}, the agent is systematically
shown all non-empty ROA-plays, and for
each ROA-play, the agent either types ``Looks like me'' (any action $>0$)
or ``Doesn't look like me'' ($0$). When shown the non-empty ROA-play
$p'\frown a'$, the agent is rewarded if and only if the agent
correctly determines whether or not $a'$
is the action the agent itself would take in response to $p'$.
Thus, the agent is incentivized to self-reflect in order to ask, ``If I experienced the
observations and rewards in that ROA-prompt, would I act that way?''

The following example is partly motivated by \cite{yampolskiy2012ai}.

\begin{example}
\label{otheraspectsexample}
    (Recognizing other aspects of oneself)
    All the below environments are similar to Example \ref{selfrecognitionexample},
    and we describe them informally to avoid technical details.
    \begin{itemize}
        \item
        (Supervised learning)
        Assume there is a canonical, computable function $f$ which transforms
        each RL agent $A$ into a supervised learning agent $f(A)$. By a \emph{supervised
        learning trial} we mean a quadruple $\langle L,T,I,p\rangle$ where $L$ is a finite set
        of labels, $T$ is a sequence of images with labels from $L$ (a \emph{training set}),
        $I$ is an unlabeled image, and $p:L\to \mathbb Q\cap [0,1]$ is a function
        assigning to each label $\ell\in L$ a probability that $\ell$ is the correct label
        for $I$. We define an extended environment as follows.
        The agent $A$ is sytematically shown all supervised learning trials and must
        take action $>0$ (``Looks like me'') or $0$ (``Doesn't look like me''), and is
        rewarded or punished depending whether or not $f(A)$ would
        output $p$ in response to $I$ after being trained with $T$.
        \item
        (Unsupervised learning)
        Assume there is a canonical, computable function $g$ which transforms each RL
        agent $A$ into an unsupervised learning agent $g(A)$.
        By an \emph{unsupervised learning trial} we mean a triple
        $\langle n,D,C\rangle$ where $n$ is a positive integer, $D\subseteq \mathbb Q^n$
        is a finite set of $n$-dimensional points with rational coordinates, and $C$
        is a clustering of $D$.
        We define an extended environment as follows. The agent $A$ is systematically
        shown all unsupervised learning trials and must take action $>0$ (``Looks like me'')
        or $0$ (``Doesn't look like me''), and is rewarded or punished depending
        whether or not $g(A)$ would cluster $D$ into clustering $C$.
        \item
        (The Turing Test)
        Assume there is a canonical, computable function $h$ which transforms each RL
        agent $A$ into an English-speaking chatbot $h(A)$.
        By a \emph{chatbot trial} we mean a sequence of strings of English characters.
        We define an extended environment as follows. The agent $A$ is systematically
        shown all chatbot trials and must take action $>0$ (``Looks like me'') or $0$
        (``Doesn't look like me''), and is rewarded or punished accordingly depending
        whether or not even-numbered strings in the chatbot trial are what $h(A)$ would
        say in response to the user saying the odd-numbered strings.
        \item
        (Adversarial sequence prediction) Similar to the above environments, assuming
        a canonical computable function which transforms each RL agent into a predictor
        in the game of adversarial sequence prediction \cite{hibbard2008adversarial}
        \cite{hibbard}.
        \item
        (Mechanical knowing agent) Assume there is a canonical, computable function
        $i$ which transforms each RL agent $A$ into a code $i(A)$ of a computably
        enumerable set of sentences in the language of Epistemic Arithmetic
        \cite{shapiro}; $i(A)$ is thought of as a mechanical knowing agent
        \cite{carlson}. We define an extended environment as follows. The agent $A$
        is systematically shown all sentences in the language of Epistemic Arithmetic,
        with repetition, in such a way that each sentence is shown infinitely often.
        Upon being shown sentence $\phi$ for the $n$th time, the agent must take action
        $>0$ (``I know $\phi$ is true'')
        or $0$ (``I'm not sure if $\phi$ is true'')
        and is rewarded if and only if $\phi$ is
        enumerated by $i(A)$ in $\leq n$ steps of computation.
    \end{itemize}
\end{example}

The extended environments in Example \ref{otheraspectsexample} incentivize the agent
to self-reflect, asking itself questions like ``Does that look like how I would
classify that image, given that training set?'' or ``Does that look like how I would
cluster that set of points?'' or ``Does that conversation participant say the same things
I would say?'' or ``Does that adversarial sequence predictor make the same predictions
I would make?'' or ``Does that mechanical knowing agent know the same things I know?''

\section{A Meta Example}

In this section we will give an example in which the agent is incentivized to self-reflect
on the subject of its own reinforcement learning performance.
Recall that ZFC is the axiomatization of set theory which serves as the
foundation of mathematics for almost all working mathematicians. The consequences of
ZFC---that is, the theorems which ZFC proves---are computably enumerable, which means
there is a Turing machine which systematically enumerates all of them and nothing else.

\begin{definition}
    Suppose $T$ is an agent, $e$ is an extended environment, and $R\in\mathbb Q$ is
    a rational number. We say that \emph{$T$ gets total reward $R$ from $e$} if
    $\sum_{i=0}^{\infty} r_i = R$, where $(r_0,o_0,a_0,\ldots)$ is the result of
    $T$ interacting with $e$ (Definition \ref{interactiondefn}).
    We say that \emph{$T$ provably gets total reward $R$ from $e$} if
    ZFC proves that $T$ gets total reward $R$ from $e$.
\end{definition}

\begin{lemma}
\label{provablerewardslemma}
    The set of triples $(T,e,R)$ where $T$ is an agent, $e$ is an extended environment,
    $R\in\mathbb Q$, and $T$ provably gets total reward $R$ from $e$, is computably enumerable.
\end{lemma}

\begin{proof}
    Follows from the fact that the consequences of ZFC are computably enumerable.
    \qed
\end{proof}

\begin{example}
\label{metaexample}
    (Reinforcement Learning itself, as an environment)
    We define an extended environment $e$ as follows.
    Let $X$ be a Turing machine which enumerates all triples
    $(T,e,R)$ as in Lemma \ref{provablerewardslemma}.
    Write $(T_0,e_0,R_0),(T_1,e_1,R_1),\ldots$ for the outputs of $X$.
    Suppose $T$ is an agent and
    $p$ is an ROA-play.
    Then $e$ will compute $e(T,p)$ as follows.

    Let $S=\{n\in\mathbb N\,:\,T_n=T\}$ be the set of indices of
    triples produced by $X$ in which the agent is $T$.
    Write $S$ as $S=\{s_0,s_1,\ldots\}$ (note that $S$ is infinite because
    there are infinitely many environments which always give reward $0$ and from which
    ZFC proves that $T$ would get total reward $0$).
    Thus, $(T_{s_n},e_{s_n},R_{s_n})=(T,e_{s_n},R_{s_n})$ is the $n$th triple enumerated
    by $X$ in which the agent is $T$. Note that $e$ can compute $(T,e_{s_n},R_{s_n})$
    and $(T,e_{s_{n+1}},R_{s_{n+1}})$ by
    simply running $X$ as long as necessary.

    If $p=\langle\rangle$ then let $e(T,p)=\langle 0, e_{s_0}\rangle$.
    But assume $p\not=\langle\rangle$.
    Write 
    \[
        p=\langle r_0,o_0,a_0,\ldots,r_n,o_n,a_n\rangle.
    \]

    Considered as an action, let $0$ denote ``play the environment''
    and let each natural $>0$ denote ``do not play the environment''.
    Having computed $(T,e_{s_n},R_{s_n})$ and $(T,e_{s_{n+1}},R_{s_{n+1}})$,
    $e$ shall output $e(T,p)=\langle r,o\rangle$, where:
    \begin{align*}
        o &= e_{s_{n+1}},\\
        r &=
        \begin{cases}
            R_{s_n} &\mbox{if $a_n=\mbox{``play the environment''}$,}\\
            0 &\mbox{otherwise.}
        \end{cases}
    \end{align*}
\end{example}

In Example \ref{metaexample}, the agent is shown source codes of many different
environments, and for each environment, the agent can choose to either ``play the
environment'' or ``not play the environment''. If the agent chooses to ``play the
environment'', then the agent instantly receives, as one single reward,
the total reward which the agent would
accumulate if it were to play in that environment for all eternity. Otherwise, if
the agent chooses to ``not play the environment'', the agent gets reward $0$.
The environments which are shown to the agent are exactly those environments such
that ZFC determines which total reward the agent would receive thereby (this is
what makes the whole exercise computable, because the consequences of ZFC are
themselves computably enumerable). The agent is incentivized to self-reflect and think:
``If I were to play the observed environment for all eternity, would I ultimately
end up with a positive or a negative total reward?''

In \cite{alexander2020archimedean} we argued that the real number system is probably not
flexible enough to accurately model the rewards of more general environments where
rewards from other number systems are permitted.
Example \ref{metaexample} could be made even more interesting if we changed the
background reinforcement learning model to allow rewards from various non-Archimedean
number systems. Those details, however, are beyond the scope of this paper.

\section{Measuring Self-awareness}

As an application, self-awareness-incentivizing extended environments could be used
to quantify the extent to which different RL agents are self-aware or capable
of self-reflection. Measurements could either be concrete and practical and
approximate, or theoretical and abstract and universal. The former could be useful
for classifying specific concrete agents (whose source-code is available to us), the
latter could be useful for proving
general theorems about self-reflection, self-awareness, intelligence, and perhaps
even consciousness.

In order to concretely measure the self-awareness of a specific agent (whose source-code
is available to us), we would simply simulate the agent's interaction with some battery
of extended environments that incentivize self-awareness, and see how the agent performs.
This approach is crude, because an agent can always perform poorly (resp.\ well)
in any finite set of environments by chance, despite being quite
self-aware (resp.\ self-unaware) in general. In the same way, computer programs can
perform well on IQ tests despite being dumb \cite{sanghi2003computer}.
Despite the crudeness of this approach, it could be very useful for obtaining
empirical insight into questions about the self-awareness, or lack thereof, of
various entities. For example, in order to get some empirical insight into the
self-awareness, or lack thereof, of an NLP system like GPT-3
\cite{chalmers}, one could twist that system into an RL agent (by means of template
programming) and see how well said RL agent performs in extended environments that
incentivize self-awareness.

It might even be possible to use extended environments to test the self-awareness
of living creatures such as lab mice. At least it would be, if a given species of
lab mice were entirely ``nurture'', as opposed to ``nature''. What we mean by a
species being entirely ``nurture'' is that any two members of that species,
if raised through identical life circumstances, would act similarly in identical environments.
Thus, if a species were entirely nurture, and if two specimens were born in identical
rooms, and experienced identical or near-identical lives\footnote{Possibly even
while still in the womb. To quote Plato: ``But it's hardly surprising you haven't
heard of these athletics of the embryo. It's a curious subject, but I'd like to
tell you about it'' \cite{platolaws}.},
and were then placed in identical mazes, then they would perform identically.
Given such a species, it would be possible to see how members perform in extended
environments, albeit perhaps at great expense. One would have to carefully raise many
similar specimens through identical or near-identical lives, diverging only at key
points in order to compute the underlying $T$ action-function on different ROA-prompts.
By seeing how the underlying $T$ agent performs on well-chosen extended environments,
we could get an idea of how self-aware those specimens were.


\bibliographystyle{splncs04}
\bibliography{intro}

\end{document}