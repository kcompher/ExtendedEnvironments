\documentclass[runningheads]{llncs}

\newtheorem{mytheorem}{Theorem}
\newtheorem{mylemma}[mytheorem]{Lemma}
\newtheorem{myquestion}[mytheorem]{Question}
\newtheorem{myexample}[mytheorem]{Example}
\newtheorem{myproposition}[mytheorem]{Proposition}
\newtheorem{mycorollary}[mytheorem]{Corollary}
\newtheorem{mydefinition}[mytheorem]{Definition}
\newtheorem{myprinciple}[mytheorem]{Principle}

\begin{document}

\title{Extending environments to incentivize self-reflection in reinforcement learning}
\titlerunning{Extending environments to incentivize self-reflection}

\author{Samuel Allen Alexander\inst{1}\orcidID{0000-0002-7930-110X}}
\authorrunning{S.\ A.\ Alexander}
\institute{The U.S.\ Securities and Exchange Commission
\email{samuelallenalexander@gmail.com}
\url{https://philpeople.org/profiles/samuel-alexander/publications}}

\maketitle

\begin{abstract}
    Fill this in.
\end{abstract}

\section{Introduction}

In building the road to AGI, we must appeal AGI's unique properties.
Indeed, if we built a road using only more general properties, which hold of other things
as well as of AGIs, then how could it be a road to AGI? It would be a road to a
more general class of things, not to AGI. Take reinforcement learning (RL) for example, in
which an agent interacts with a certain type of reward-giving environment (hereafter called
a \emph{traditional RL environment}).
An AGI could interact with such
a traditional RL environment, but so could various other creatures: humans, animals,
human societies. If there is nothing unique about how an AGI interacts with
traditional RL environments,
then how could traditional RL environments possibly lead to AGI?
Thinking along these lines motivated us to consider
special extended environments in which AGIs can participate, but in which humans or animals
can not realistically participate.

AGIs (as we envision them here) have a key property which uniquely allows them to
participate in extended environments in which other actors could not practically
participate. Namely, we envision an AGIs as acting deterministically according to
a well-defined computer program. This is crucial because, unlike a human or animal
actor, such a computer program can be perfectly simulated and forked. Whereas a
traditional RL environment chooses rewards and updates states based solely on the
actions which an agent takes, our extended environments will have access to the AGI's
source-code, allowing the environment to choose rewards and states based on, for example,
what the agent \emph{would have done} in various situations. For example, a chess-playing
environment, when deciding whether to make a move which would potentially expose it to
checkmate, can, as part of its deliberations, simulate the AGI opponent in order to
determine whether or not the AGI opponent would take advantage of said opportunity.

We will give examples of extended environments designed to incentivize RL agents to
recursively engage in self-reflection, in the sense that any agent who does not so
engage could not possibly perform well in said environments.
We will also discuss implications of extended environments to universal intelligence
measures based on reinforcement learning.

\bibliographystyle{splncs04}
\bibliography{intro}

\end{document}