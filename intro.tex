\documentclass[runningheads]{llncs}
\usepackage{amsfonts}
\usepackage{amsmath}

\newtheorem{mytheorem}{Theorem}
\newtheorem{mylemma}[mytheorem]{Lemma}
\newtheorem{myquestion}[mytheorem]{Question}
\newtheorem{myexample}[mytheorem]{Example}
\newtheorem{myproposition}[mytheorem]{Proposition}
\newtheorem{mycorollary}[mytheorem]{Corollary}
\newtheorem{mydefinition}[mytheorem]{Definition}
\newtheorem{myprinciple}[mytheorem]{Principle}

\begin{document}

\title{Extending environments to incentivize self-reflection
in reinforcement learning}
\titlerunning{Extending environments to incentivize self-reflection}

\author{Samuel Allen Alexander\inst{1}\orcidID{0000-0002-7930-110X}}
\authorrunning{S.\ A.\ Alexander}
\institute{The U.S.\ Securities and Exchange Commission
\email{samuelallenalexander@gmail.com}
\url{https://philpeople.org/profiles/samuel-alexander/publications}}

\maketitle

\begin{abstract}
    We consider an extended notion
    of reinforcement learning environment, in which the environment is able
    to simulate the agent who is participating in the environment. We give
    examples of some such extended environments which seem to incentivize
    various different types of self-reflection or self-awareness.
\end{abstract}

\section{Introduction}

In this paper, we consider computable agents in a deterministic variant of the reinforcement
learning (RL) framework. Such agents are intended to interact with deterministic RL
environments, where they take actions and receive rewards and observations in
response to those actions. Essentially, such an environment is basically a function $e$
which outputs a reward-observation pair
$\langle r,o\rangle=e(a_0,\ldots,a_n)$ in response to an action-sequence $(a_0,\ldots,a_n)$.

However, there is another type of environment in which these
same RL agents can interact just as well. Essentially, we define an \emph{extended
environment} to be a function $e$ which outputs a reward-observation pair
$\langle r,o\rangle=e((a_0,\ldots,a_n), T)$ in response to an action-sequence
$(a_0,\ldots,a_n)$ along with a Turing machine $T$ which computes an agent.
Intuitively, this should be thought of as follows: when the agent enters the environment,
the environment is made aware of the source-code of the agent, and can use that
source-code when computing what rewards and observations to give to the agent in response
to the agent's actions.

For example, the environment might be a game consisting of a
labyrinth where the agent wanders from
room to room, each room containing a treasure, and most (but not all) rooms containing
a guard. The agent can only see one room at a time, and cannot look ahead into adjacent
rooms. In a room with no guard, the agent can choose to take the treasure, in which
case the agent receives a positive reward.
If the player chooses to take the treasure in a guarded room,
then, by simulating the agent, the guard determines: ``If
I (the guard) were not present in this room, would the agent take the treasure?''
If the answer is ``yes'', then the guard prevents the agent from taking the treasure,
and zaps the agent (which the agent feels as a negative reward, i.e., a punishment).
If the answer is ``no'', then the guard allows the agent to take the treasure (and the
agent is rewarded)\footnote{This environment bears some similarity to Newcomb's
Paradox \cite{nozick1969newcomb}.}.

The above environment would apparently be impossible (or prohibitively expensive)
for a human to play, due to the difficulty of the guards simulating the human player.
But there is no reason why the above game could not be played by an RL agent (the
RL agent's source-code being fed to the game-engine beforehand). Clearly this kind of
extended environment is not the kind of environment the RL agent is intended to be
applied to, however, such environments could be useful on the path to Artificial
General Intelligence (AGI) because they seem to incentivize self-reflection: in order
to perform well in the above example, the agent would evidently need to achieve some
degree of self-awareness in order to figure out the underlying pattern behind the
guards.

We will give examples of some interesting
extended environments of the above kind, designed to incentivize RL agents to
recursively engage in self-reflection in various ways.
We conjecture that traditional RL techniques will not perform well in these extended
environments, because traditional RL techniques do not involve any sort of
self-reflection. Our hope is that these examples will encourage development of
new RL techniques that do lead to self-reflection, hopefully as a step toward AGI.

\section{Preliminaries}

\begin{definition}
(Plays and prompts)
    \begin{enumerate}
        \item
        By an \emph{ROA-play}, we mean either the empty sequence $\langle\rangle$,
        or else a sequence of the form
        \[
            r_0,o_0,a_0,\ldots,r_k,o_k,a_k
        \]
        where each $r_i\in\mathbb Q$ (thought of as a \emph{reward}),
        each $o_i\in\mathbb N$ (thought of as an \emph{observation}),
        and each $a_k\in \mathbb N$ (thought of as an \emph{action}).
        \item
        By an \emph{ROA-prompt}, we mean a sequence of the form
        $s\frown r\frown o$
        where $s$ is an ROA-play, $\frown$ denotes concatenation,
        $r\in\mathbb Q$ (thought of as a \emph{reward}),
        and $o\in\mathbb N$ (thought of as an \emph{observation}).
    \end{enumerate}
\end{definition}

\begin{definition}
\label{agentandenvironment}
(Agents and environments)
    \begin{enumerate}
    \item An \emph{agent} is a Turing machine which
    halts whenever it is run on an ROA-prompt, outputting
    an action $a\in\mathbb N$.
    \item An \emph{extended environment} is a Turing machine $e$ such that:
        \begin{enumerate}
            \item
            For every agent $T$, for every
            ROA-play $s$,
            when $e$ is run on input $\langle T,s\rangle$, $e$ halts on that input,
            outputting a pair $\langle r,o\rangle$ where $r\in\mathbb Q$ (thought of
            as a reward) and $o\in\mathbb N$ (thought of as an observation).
        \end{enumerate}
    \end{enumerate}
\end{definition}

There is a subtle nuance in Definition \ref{agentandenvironment}. Should the agent's
next action depend on the entire history (including prior actions), or only on prior
rewards and observations? One could argue that
the agent's next action needn't depend on its own past actions, since its own past actions
can be inferred from past rewards and observations.
Normally, it would not matter much whether
or not the agent's next action depend on its own past actions\footnote{In
\cite{alexander2019intelligence}, we
formalize agents whose actions depend only on past rewards and observations.
Legg and Hutter give a formalization where the agent's next action does explicitly
depend on its past actions \cite{legg2007universal}.}. In formalizing examples of extended
environments that incentivize self-reflection, we have found it convenient for the agent's
next action to formally depend on past actions. Perhaps this reflects that known
conscious agents (e.g.\ humans)
evidently do \emph{not} carefully re-compute their own
past actions from remembered observations and rewards, but instead, a human maintains
memories of her past actions as well, regardless whether doing so is formally superfluous.


\begin{definition}
    Suppose $T$ is an agent and $e$ is an extended environment.
    The \emph{result of $T$ interacting with $e$} is the infinite
    reward-observation-action sequence
    \[r_0,o_0,a_0,r_1,o_1,a_1,\ldots\]
    (each $r_i\in\mathbb Q$, $o_i,a_i\in\mathbb N$)
    defined inductively as follows.
    \begin{itemize}
        \item $r_0$ and $o_0$ are obtained by computing $e$ on
        $\langle T,\langle\rangle\rangle$.
        \item $a_0$ is the output of $T$ on $\langle r_0,o_0\rangle$.
        \item For $i>0$, $r_i$ and $o_i$ are obtained by computing $e$
        on
        \[\langle T,\langle r_0,o_0,a_0,\ldots,r_{i-1},o_{i-1},a_{i-1}\rangle\rangle.\]
        \item For $i>0$, $a_i$ is obtained by computing $T$ on
        \[\langle r_0,o_0,a_0,\ldots,r_{i-1},o_{i-1},a_{i-1},r_i,o_i\rangle.\]
    \end{itemize}
\end{definition}

One important implication of extended environments is that they further divide
the (already divided) ways of measuring intelligence of RL agents. Intelligence
measures
\cite{alexander2019intelligence} \cite{hernandez} \cite{legg2007universal}
which aggregate performance over traditional environments only measure
an agent's intelligence over those environments. The same measures could easily
be extended to also take extended environments into account, perhaps providing
measures which better capture agents' self-reflection ability.

\section{Examples of Self-reflection-incentivizing Environments}

In this section, we give examples of some interesting extended environments which seem
to incentivize various forms of self-reflection. We are inspired by various libraries of
traditional RL environments \cite{bellemare2013arcade}
\cite{beyret2019animal} \cite{brockman2016openai} \cite{chollet2019measure}
\cite{cobbe2020leveraging}. Some of our examples do not provide individual environments,
but rather, procedures for obtaining new environments from old (these could be iterated,
in order to build up environments incentivizing deeper and deeper nested self-reflection).

\begin{example}
\label{rewardagentforignoringrewardsexample}
    (Reward Agent for Ignoring Rewards)
    For each ROA-prompt $p$, let $p^0$ be the ROA-prompt equal to $p$ except that
    all rewards are $0$.
    For any environment $e$, we define a new environment
    $e'$ as follows
    (where $T$ is a Turing machine, $p$ is an ROA-prompt, and $a\in \mathbb N$ is thought of as
    the agent's action in response to $p$):
    \begin{align*}
        e'(T,\langle\rangle) &= e(T,\langle\rangle)\\
        e'(T,p\frown a)
        &= \langle r,o\rangle,
    \end{align*}
    where $o$ is the observation component of
    $e(T,p\frown a)$ (if $e$ does not halt on input $\langle T,p\frown a\rangle$
    (resp.\ $\langle T,\langle\rangle\rangle$)
    then neither does $e'$) and
    \[
        r =
        \begin{cases}
            1 & \mbox{if $a=T(p^0)$,}\\
            -1 & \mbox{if $a\not=T(p^0)$}
        \end{cases}
    \]
    (if $T$ does not halt on $p^0$ then $e'$ does not halt on
    $\langle T,p\frown a\rangle$).
\end{example}

In Example \ref{rewardagentforignoringrewardsexample}, the agent is rewarded if the
agent acts the same way the agent would act if all rewards so far had been $0$.
Otherwise, the agent is punished. Thus, paradoxically, the agent is rewarded for
ignoring rewards. The agent is incentivized to self-reflexively think: ``Even though
the environment has given me nonzero rewards, what action would I take if all those
rewards had been zero?''

\begin{example}
\label{selfinsertionexample}
    (Reward Agent for Self-Inserting)
    Fix a canonical computable bijection
    $o\mapsto \hat o$
    from $\mathbb N$ to $\mathbb Q\times \mathbb N$:
    thus, every observation $o$ encodes a reward-observation pair
    $\hat o = \langle r',o'\rangle$, and every reward-observation pair
    is encoded by some such $o$.
    For any environment $e$, we define
    a new environment $e'$ as follows
    (where $T,p,a$ are as in Example \ref{rewardagentforignoringrewardsexample},
    and with similar non-halting caveats
    as Example \ref{rewardagentforignoringrewardsexample}):
    \begin{align*}
        e'(T,\langle\rangle) &= e(T,\langle\rangle)\\
        e'(T,p\frown a) &= \langle r,o\rangle,
    \end{align*}
    where $o$ is such that $\hat o = e(T,p\frown a)$ and
    \[
        r =
        \begin{cases}
            1 & \mbox{if $a=T(p')$,}\\
            -1 & \mbox{if $a\not=T(p')$}
        \end{cases}
    \]
    where
    $p'$ is the ROA-prompt obtained from $p$
    by replacing each reward-observation pair
    $\ldots,r_i,o_i,\ldots$ by the reward-observation
    pair $\ldots,\widehat{o_i},\ldots$.
\end{example}

In Example \ref{selfinsertionexample}, one might imagine $e'$ as a room containing nothing
but an arcade game $e$. There is nothing for the agent in the room to do
except play this arcade game.
When played, the arcade game
visually displays rewards, but the agent merely observes them, and does not
``feel'' them. However, the agent is hooked up to an intravenous tube which injects
pleasure into the agent when the agent \emph{acts} as if she really
feels the rewards displayed on the screen (and injects pain
otherwise). In this way, the agent is incentivized
to self-identify with the protagonist in the video-game, self-reflexively asking,
``Which action would I take if those displayed rewards were real?''

\begin{example}
\label{incentivetoincentivizeexample}
    (Incentive to Incentivize)
    We define an environment $e$ as follows
    (where $T$ is a Turing machine and $r_0,o_0,a_0,\ldots,r_n,o_n,a_n$
    is an ROA-play,
    with similar non-halting caveats as in
    Example \ref{rewardagentforignoringrewardsexample}).
    \begin{align*}
        e(T,\langle\rangle) &= \langle 0, 0\rangle\\
        e(T,\langle r_0,o_0,a_0,\ldots,r_n,o_n,a_n\rangle) &= \langle r, 0\rangle,
    \end{align*}
    where
    \[
        r =
        \begin{cases}
            1 & \mbox{if $T(p')=0$},\\
            0 & \mbox{if $T(p')\not=0$}
        \end{cases}
    \]
    where $p'$ is the ROA-prompt $(r'_0,o'_0,a'_0,\ldots,r'_{n+1},o'_{n+1})$
    where $r'_0=0$, each $r'_{i+1}=a_i$,
    each $o'_i=0$, and each
    $a'_i=T(r'_0,o'_0,a'_0,\ldots,r'_i,o'_i)$.
\end{example}

In Example \ref{incentivetoincentivizeexample}, the agent observes
actions, and the agent must act by
choosing rewards for those observed actions.
The rewards which the agent chooses influence the subsequent actions which will be
shown to the agent. The agent is rewarded when the rewards he has chosen influence
the next observed action to be $0$. Unknown to the agent---but an intelligent
self-aware agent should eventually notice the pattern---the way the observed actions
are determined is by giving the agent's chosen rewards to a simulated copy of said agent.
Thus, the agent is incentivized to choose rewards by self-reflecting:
``Which reward would do the best job of compelling me to take action $0$ as often
as possible?'' We might imagine the agent playing a video-game in which he sees himself
in front of a keyboard. The video-game copy-agent types
``$100$'', and then a prompt appears on the screen saying, ``Which reward will you give this
worker for typing $100$ just now?'' The
agent responds by choosing some reward, and sees an animation
of the reward being given to the video-game copy-agent. The video-game copy-agent
then types ``$0$'', and immediately the true agent is rewarded for getting the video-game
copy-agent to type $0$. Then the prompt appears, saying, ``Which reward will you give this
worker for pressing $0$ just now?'' And so on
forever\footnote{Example \ref{incentivetoincentivizeexample}
is interesting in that the agent, desiring the clone to take action $0$
as often as possible, is incentivized to choose harsh punishments when the clone takes
nonzero actions.
If punishments are limited to $\mathbb Q$, then the agent faces a dilemma
similar to one in RL cancer treatment applications.
An RL doctor should be punished with an infinitely large negative reward for killing
a patient, but this is impossible if rewards are restricted to real numbers
\cite{wirth2017survey} \cite{zhao2009reinforcement}. Similarly, the agent in Example
\ref{incentivetoincentivizeexample} would probably like to choose infinitely large
rewards, if possible, when its clone from takes action $0$. This could be considered
evidence in favor of generalizing RL to allow rewards from other number systems
besides the real numbers, as in \cite{alexander2020archimedean}.}.

Many other interesting examples could be given. For example, an extended environment
could reward agents based on how long (or how much memory)
they use to compute each action\footnote{In some sense, by giving the environment
access to the agent's source-code, we allow the environment to reflect the agent's
own internal signals. Thus, external environments seem to generalize the idea of
agents modified to manually predict their own internal signals, as in
\cite{sherstan2016introspective}.}. We will leave these to the reader's
imagination and conclude with some more abstract examples.

\begin{example}
\label{selfrecognitionexample}
    (Incentivizing Self-recognition)
    Let $p_0,p_1,\ldots$ be a canonical computable enumeration of all non-empty ROA-plays.
    We define an environment $e$ as follows (where $T,p,a$
    are as in Example \ref{rewardagentforignoringrewardsexample}, and with appropriate
    non-halting caveats).
    \begin{align*}
        e(T,\langle\rangle) &= \langle 0, p_0\rangle\\
        e(T,p\frown a) &= \langle r, p_{n}\rangle
    \end{align*}
    where $n=\frac{\mbox{len}(p\frown a)}{3}$ is the number of actions in $p\frown a$ and where
    \[
        r =
        \begin{cases}
            1 &\mbox{if $a>0$ and $a'=T(p')$,}\\
            1 &\mbox{if $a=0$ and $a'\not=T(p')$,}\\
            0 &\mbox{otherwise}
        \end{cases}
    \]
    where $p_{n-1}=p'\frown a'$.
\end{example}

In Example \ref{selfrecognitionexample}, the agent is systematically
shown all non-empty ROA-plays, and for
each ROA-play, the agent either types ``Looks like me'' (any action $>0$)
or ``Doesn't look like me'' ($0$). When shown the non-empty ROA-play
$p'\frown a'$, the agent is rewarded if and only if the agent
correctly determines whether or not $a'$
is the action the agent itself would take in response to $p'$.
Thus, the agent is incentivized to self-reflect in order to ask, ``If I experienced the
observations and rewards in that OAR-sequence, would I act that way?''

The following example is partly motivated by \cite{yampolskiy2012ai}.

\begin{example}
\label{otheraspectsexample}
    (Recognizing other aspects of oneself)
    All the below environments are similar to Example \ref{selfrecognitionexample},
    and we describe them informally to avoid technical details.
    \begin{itemize}
        \item
        (Supervised learning)
        Assume there is a canonical, computable function $f$ which transforms
        each RL agent $A$ into a supervised learning agent $f(A)$. By a \emph{supervised
        learning trial} we mean quadruple $\langle L,T,I,p\rangle$ where $L$ is a finite set
        of labels, $T$ is a sequence of images with labels from $L$ (a \emph{training set}),
        $I$ is an unlabeled image, and $p:L\to \mathbb Q\cap [0,1]$ is a function
        assigning to each label $\ell\in L$ a probability that $\ell$ is the correct label
        for $I$. We define an extended environment as follows.
        The agent $A$ is sytematically shown all supervised learning trials and must
        take action $1$ (``Looks like me'') or $0$ (``Doesn't look like me''), and is
        rewarded or punished accordingly depending whether or not $f(A)$ would
        output $p$ in response to $I$ after being trained with $T$.
        \item
        (Unsupervised learning)
        Assume there is a canonical, computable function $g$ which transforms each RL
        agent $A$ into an unsupervised learning agent $g(A)$.
        By an \emph{unsupervised learning trial} we mean a triple
        $\langle n,D,C\rangle$ where $n$ is a positive integer, $D\subseteq \mathbb Q^n$
        is a finite set of $n$-dimensional points with rational coordinates, and $C$
        is a clustering of $D$.
        We define an extended environment as follows. The agent $A$ is systematically
        shown all unsupervised learning trials and must take action $1$ (``Looks like me'')
        or $0$ (``Doesn't look like me''), and is rewarded or punished accordingly depending
        whether or not $g(A)$ would cluster $D$ into clustering $C$.
        \item
        (The Turing Test)
        Assume there is a canonical, computable function $h$ which transforms each RL
        agent $A$ into an English-speaking chatbot $h(A)$.
        By a \emph{chatbot trial} we mean a sequence of strings of English characters.
        We define an extended environment as follows. The agent $A$ is systematically
        shown all chatbot trials and must take action $1$ (``Looks like me'') or $0$
        (``Doesn't look like me''), and is rewarded or punished accordingly depending
        whether or not even-numbered strings in the chatbot trial are what $h(A)$ would
        say in response to the user saying the odd-numbered strings.
        \item
        (Adversarial sequence prediction) Similar to the above environments, assuming
        a canonical computable function which transforms each RL agent into a predictor
        in the game of adversarial sequence prediction \cite{hibbard2008adversarial}
        \cite{hibbard}.
        \item
        (Mechanical knowing agent) Assume there is a canonical, computable function
        $i$ which transforms each RL agent $A$ into a code $i(A)$ of a computably
        enumerable set of sentences in the language of Epistemic Arithmetic
        \cite{shapiro}; $i(A)$ is thought of as a mechanical knowing agent
        \cite{carlson}. We define an extended environment as follows. The agent $A$
        is systematically shown all sentences in the language of Epistemic Arithmetic,
        with repetition, in such a way that each sentence is shown infinitely often.
        Upon being shown sentence $\phi$ for the $n$th time, the agent must take action
        $1$ (``I know $\phi$ is true'')
        or $0$ (``I'm not sure if $\phi$ is true'')
        and is rewarded if and only if $\phi$ is
        enumerated by $i(A)$ in $\leq n$ steps of computation.
    \end{itemize}
\end{example}

The extended environments in Example \ref{otheraspectsexample} incentivize the agent
to self-reflect, asking itself questions like ``Does that look like how I would
classify that image, given that training set?'' or ``Does that look like how I would
cluster that set of points?'' or ``Does that conversation participant say the same things
I would say?'' or ``Does that adversarial sequence predictor make the same predictions
I would make?'' or ``Does that mechanical knowing agent know the same things I know?''

\bibliographystyle{splncs04}
\bibliography{intro}

\end{document}